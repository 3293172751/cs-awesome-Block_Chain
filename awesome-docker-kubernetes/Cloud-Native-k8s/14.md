+ [author](http://nsddd.top)

# 第14节 k3s

<div><a href = '13.md' style='float:left'>⬆️上一节🔗  </a><a href = '15.md' style='float: right'>  ⬇️下一节🔗</a></div>
<br>

> ❤️💕💕新时代拥抱云原生，云原生具有环境统一、按需付费、即开即用、稳定性强特点。Myblog:[http://nsddd.top](http://nsddd.top/)

---
[[toc]]

[toc]

## k3s介绍

::: tip k3s — 微型kubernets发行版
k3s是经CNCF一致性认证的Kubernetes发行版，专为物联网及边缘计算设计。

+ [官方](https://www.rancher.cn/k3s/)
+ [文档](https://docs.rancher.cn/)
+ [开源地址](https://github.com/k3s-io/k3s/)

**技术亮点：**

+ 单进程架构简化部署
+ 移除各种非必要的代码，减少资源占用
+ `TLS` 证书管理
+ 内置 `Containerd`
+ 内置自运行 `rootfs`
+ 内置 `Helm Chart` 管理机制
+ 内置 `L4/L7 LB` 支持



**适合场景：**

+ 边缘计算-Edge
+ 物联网-IoT
+ CI
+ Development
+ ARM
+ 嵌入 K8s

:::



**架构图：**

![k3s下载](http://sm.nsddd.top/smhow-it-works-k3s.svg)



## k3s和k8s区别

::: tip 
K3s是一个独立的服务器，与K8s不同，它是Kubernetes集群的一部分。K8s依靠CRI-O来整合Kubernetes与CRI（容器运行时接口），而K3s使用CRI-O与所有支持的容器运行时兼容。K8s使用kubelet来调度容器，但K3s使用主机的调度机制来调度容器。

:::

k3s有比k8s更严格的安全部署，因为其攻击面小。k3s的另一个优势是，它可以减少安装、运行或更新Kubernetes集群所需的依赖性和步骤。

::: warning 如何封装 k8s –> k3s 中
原理就是，将 `K8S` 的相关组件封装到 `K3s` 的二进制文件中去，然后启动这二进制文件就可以启动一个成熟的 `K8S` 集群。在下面🔽 我们可以看到 `K3s` 和 `K8S` 的架构基本差不多，其中 `k3s-server` 对应这个 `control-plane`，而 `k3s-agent` 对应着 `node` 节点。

可以看到 `k3s` 中使用的默认存储是 `SQLite`(自带)，且默认的网络使用的是 `Flannel`(自带)。当服务端和客户端都启动之后，通过 `Tunnel-Proxy` 这个组件进行通信，通过这个通道去管理网络流量。在 `agent` 节点中，通过 `kubelet` 操作 `contaninerd` 来创建对应 `Pod`。 

:::



## 架构

k3s架构就是把k8s核心组件封装成二进制~

k3s分为`k3s server` 和 ` k3s agent`：

+  k3s server 只有一个进程体
+  k3s agent 分为两个进程体，其中一个是 Contrainerd，负责管理运行容器

> 在下面也可以深刻理解到



**架构详解：**

::: details 架构讲解
k3s算是对k8s的架构和生态进行一部分精华和缩进

**单节点架构：**

K3s 单节点集群的架构如下图所示，该集群有一个内嵌 SQLite 数据库的单节点  `K3s server` 。

在这种配置中，每个  `agent` 节点都注册到同一个  `server` 节点。K3s 用户可以通过调用  `server` 节点上的 K3s API 来操作 Kubernetes 资源。

**单节点 `K3s server` 的架构：**

![img](http://sm.nsddd.top/sm1660616402558126.png)

**高可用架构：**

虽然单节点 k3s 集群可以满足各种用例，但对于 Kubernetes control-plane 的正常运行至关重要的环境，您可以在高可用配置中运行 K3s。一个高可用 K3s 集群由以下几个部分组成：

+ **`K3s server` 节点** ：两个或更多的`server`节点将为 Kubernetes API 提供服务并运行其他 control-plane 服务
+ **外部数据库** ：与单节点 k3s 设置中使用的嵌入式 `SQLite` 数据存储相反，高可用 K3s 需要挂载一个 `external database` 外部数据库作为数据存储的媒介。

**K3s高可用架构（非嵌入式架构图）：**

![img](http://sm.nsddd.top/sm1660616476551520.png)

**高可用架构（嵌入式架构）：**

> 注意：高可用结构同样可以使用**嵌入式数据库**
>
> ⚠️ 区别：
>
> **嵌入数据库是指数据在内存中数据库，英文称为–embedded**，又称in-memory embedded database，如H2, HSQL and Derby databases。
>
> **非嵌入式数据库是指数据在磁盘中的数据库**，如MariaDB, MySQL and Oracle。

![image-20221117173105788](http://sm.nsddd.top/smimage-20221117173105788.png)

**固定  `agent` 节点的注册地址：**

在高可用   `K3s server`  配置中，每个节点还必须使用固定的注册地址向 Kubernetes API 注册，注册后， `agent` 节点直接与其中一个  `server` 节点建立连接：

<img src="http://sm.nsddd.top/sm1660616545857393.svg" alt="k3s-production-setup" style="zoom: 25%;" />

**注册  `agent` 节点：**

 `agent` 节点用`k3s agent`进程发起的 websocket 连接注册，连接由作为代理进程一部分运行的客户端负载均衡器维护。

 `agent` 将使用节点集群 `secret` 以及随机生成的节点密码向   `K3s server`  注册，密码存储在 `/etc/rancher/node/password`路径下。 `K3s server` 将把各个节点的密码存储为 `Kubernetes secrets`，随后的任何尝试都必须使用相同的密码。节点密码秘密存储在`kube-system`命名空间中，名称使用模板`<host>.node-password.k3s`。

> **注意：**
>
> + 在 K3s v1.20.2 之前，` K3s  server` 将密码存储在`/var/lib/rancher/k3s/server/cred/node-passwd`的磁盘上。
> + 如果您删除了  `agent` 的`/etc/rancher/node`目录，则需要为该  `agent` 重新创建密码文件，或者从  `server` 中删除该条目。
> + 通过使用`--with-node-id`标志启动 `  K3s server` 或 agent，可以将唯一的节点 ID 附加到主机名中。

**自动部署的清单：**

位于目录路径`/var/lib/rancher/k3s/server/manifests` 的清单在构建时被捆绑到 K3s 二进制文件中，将由[rancher/helm-controller](https://github.com/k3s-io/helm-controller#helm-controller)在运行时安装。

:::



## Sqlite 和 Dqlite

我认为我在这里遇到了很多疑惑，关于 Sqlite 和 [Dqlite](https://github.com/canonical/dqlite/blob/master/README_CH.md)

::: tip dqlite
“dqlite”是“distributed SQLite”的简写，即分布式SQLite。意味着 dqlite 通过网络协议扩展 SQLite ，将应用程序的各个实例连接在一起，让它们作为一个高可用的集群，而不依赖外部数据库。
:::

我希望 runtime 可以实现 multi-master ，同样支持的嵌入式和外部DB



::: danger 
关于 单结点 扩展为 高可用 状态，或许这并不是一个很容器实现的地方，我们在前面 details 中看到单结点架构和高可用架构的区别，或许我们应该在制作 `runtime` 模块 和 `rootfs` 的时候更倾向于实现 高可用。
:::



## 新版本默认支持 etcd

::: tip
从 `v1.19.5+k3s1` 版本开始，K3s 已添加了对嵌入式 etcd 的完全支持。从 v1.19.1 到 v1.19.4 版本只提供了对嵌入式 etcd 的实验性支持。在 K3s v1.19.1 版本中，嵌入式 etcd 取代了实验性的 Dqlite。这是一个突破性的变化。请注意，不支持从实验性 Dqlite 升级到嵌入式 etcd。如果你尝试升级，升级将不会成功，并且数据将会丢失。

嵌入式 etcd (HA) 在速度较慢的磁盘上可能会出现性能问题，例如使用 SD 卡运行的 Raspberry Pi。

⚠️ 注意，如果你使用 docker 作为runtime，请小心 docker 是不认识 `+` ，如果你希望的到指定版本，请使用 ： `v1.19.5-k3s1` 

:::



## 在线 [[docs/Cloud-Native-k8s/15#第15节 k3s 补充|脚本安装]]  k3s

**安装之前请保证 k3s 的目录干净：**

```bash
 rm -rf /etc/rancher /var/lib/rancher
```

::: warning 启动k3s有多快？
一行代码搞定 — 仅需30秒，即可启动k3s：

```bash
curl -sfL https://get.k3s.io | sh -
# Check for Ready node, takes maybe 30 seconds
k3s kubectl get node

# if u in china, u can speed up the installation in the following ways
curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -
# -s 不输出任何东西  &  -f 连接失败时不显示http错误  & -L参数会让 HTTP 请求跟随服务器的重定向。curl 默认不跟随重定向。
```

> **同样你可以选择把k3s部署在docker中，这样你就可以很方便的管理k3s**
>
> `curl -sfL https://get.k3s.io | sh -` 将其 `server` 和 `agent` 都安装上了。
>
> **如何扩充结点**

**安装选项：**

+ [使用脚本安装的选项](https://docs.rancher.cn/docs/k3s/installation/install-options/_index#使用脚本安装的选项)
+ [从二进制中安装的选项](https://docs.rancher.cn/docs/k3s/installation/install-options/_index#从二进制安装的选项)
+ [K3s server 的注册选项](https://docs.rancher.cn/docs/k3s/installation/install-options/_index#k3s-server-的注册选项)
+ [K3s agent 的注册选项](https://docs.rancher.cn/docs/k3s/installation/install-options/_index#k3s-agent-的注册选项)
+ [配置文件](https://docs.rancher.cn/docs/k3s/installation/install-options/_index#配置文件)



**离线安装：**

+ [https://docs.rancher.cn/docs/k3s/installation/airgap/_index/](https://docs.rancher.cn/docs/k3s/installation/airgap/_index/)



日志查看k3s启动信息：

```bash
tail -f /var/log/syslog
# 或者
kubectl get all -n kube-system
```

:::

::: danger 相比较二进制，推荐脚本安装
虽然可以通过下载二进制文件进行服务端和工作节点的运行(`./k3s server`)，但是一旦我们退出进程，之前创建的节点也就立即销毁了，所以还是建议使用脚本进行安装。

⚠️ 多说几句：之前不是很理解，xian'z

:::





## 在线安装的解析

### 安装内容

+ `kubectl`、`crictl`、`ctr`
+ `k3s-killall.sh`、`k3s-uninstall.sh`



### 执行操作

+ 将 `kubeconfig` 文件写入到 `/etc/rancher/k3s/k3s.yaml` 里面
+ 由 `K3s` 安装的 `kubectl` 工具将自动使用该文件的配置来运行
+ 其他机器可以通过复制这个配置文件并修改 `server` 地址来操作 `K3s` 集群



### 指定版本

**我们前面默认安装最新版，或许我们可以指定版本安装：**

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.25.3 sh -
```



### 指定数据库

::: tip 场景
![image-20221124193104746](http://sm.nsddd.top/smimage-20221124193104746.png)

:::

**以MySQL为例：**

```bash
curl -sfL https://get.k3s.io | sh -s - server --datastore-endpoint='mysql://admin:Rancher2019k3s@tcp(k3s-mysql.csrskwupj33i.ca-central-1.rds.amazonaws.com:3306)/k3sdb'
# 注意database name不要加特殊字符
```

**任意节点查看node：**

```bash
kubectl get no
```



### 指定容器运行时

**运行时：**

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--docker" sh -

# Domestic mirror acceleration
curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn | INSTALL_K3S_EXEC="--docker"  sh -
```

> 这样我们可以使用 docker 来管理 k3s 

| Flag                                 | 默认值                             | 描述                                               |
| ------------------------------------ | ---------------------------------- | -------------------------------------------------- |
| `--docker`                           | N/A                                | 用 docker 代替 containerd                          |
| `--container-runtime-endpoint` value | N/A                                | 禁用嵌入式 containerd，使用替代的 CRI 实现。       |
| `--pause-image` value                | "docker.io/rancher/pause:3.1"      | 针对 containerd 或 Docker 的自定义 pause 镜像      |
| `--snapshotter` value                | N/A                                | 覆盖默认的 containerd 快照程序 (默认: "overlayfs") |
| `--private-registry` value           | "/etc/rancher/k3s/registries.yaml" | 私有镜像仓库配置文件                               |



::: details k3s安装动态

```bash
root@ubuntu:/sealos# curl -fL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 29713  100 29713    0     0   148k      0 --:--:-- --:--:-- --:--:--  149k
[INFO]  Finding release for channel stable
[INFO]  Using v1.25.3+k3s1 as release
[INFO]  Downloading hash rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/v1.25.3-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/v1.25.3-k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
################################################################################
root@ubuntu:/sealos# k3s
NAME:
   k3s - Kubernetes, but small and simple

USAGE:
   k3s [global options] command [command options] [arguments...]

VERSION:
   v1.25.3+k3s1 (f2585c16)

COMMANDS:
   server           Run management server
   agent            Run node agent
   kubectl          Run kubectl
   crictl           Run crictl
   ctr              Run ctr
   check-config     Run config check
   etcd-snapshot    Trigger an immediate etcd snapshot
   secrets-encrypt  Control secrets encryption and keys rotation
   certificate      Certificates management
   completion       Install shell completion script
   help, h          Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                     (logging) Turn on debug logs [$K3S_DEBUG]
   --data-dir value, -d value  (data) Folder to hold state (default: /var/lib/rancher/k3s or ${HOME}/.rancher/k3s if not root)
   --help, -h                  show help
   --version, -v               print the version

```

:::



## 离线安装解释

::: tip 提醒
下载离线安装脚本：https://get.k3s.io

下载**k3s**二进制文件：k3s

下载必要的`images`：

```bash
wget https://ghproxy.com/https://github.com/k3s-io/k3s/releases/download/v1.25.3%2Bk3s1/k3s-airgap-images-amd64.tar
```

> **These files are available in the [GitHub](https://github.com/k3s-io/k3s/) repository**
>
> ![image-20221109164523589](http://sm.nsddd.top/smimage-20221109164523589.png)

:::



### 步骤

**步骤 1**：部署镜像，本文提供了两种部署方式，分别是**部署私有镜像仓库**和**手动部署镜像**。请在这两种方式中选择一种执行。

**步骤 2**：安装 K3s，本文提供了两种安装方式，分别是**单节点安装**和**高可用安装**。完成镜像部署后，请在这两种方式中选择一种执行。

**离线升级 K3s 版本**：完成离线安装 K3s 后，您还可以通过脚本升级 K3s 版本，或启用自动升级功能，以保持离线环境中的 K3s 版本与最新的 K3s 版本同步。



**请按照以下步骤准备镜像目录和 K3s 二进制文件：**

> 我认为离线安装的重点在于**K3s 依赖的镜像**部分，因为 K3s 的"安装脚本"和"二进制文件"只需要下载到对应目录，然后赋予相应的权限即可，非常简单。但K3s 依赖的镜像的安装方式取决于你使用的是手动部署镜像还是私有镜像仓库，也取决于容器运行时使用的是 `containerd` 还是`docker`。
>
> 针对不同的组合形式，可以分为以下几种形式来实现离线安装：
>
> + Containerd + 手动部署镜像方式
> + Docker + 手动部署镜像方式
> + Containerd + 私有镜像仓库方式
> + Docker + 私有镜像仓库方式

1. 从[K3s GitHub Release](https://github.com/rancher/k3s/releases)页面获取你所运行的 K3s 版本的镜像 tar 文件。(**airgap-images**)

2. 将 tar 文件放在`images`目录下，例如：

   ```bash
   sudo mkdir -p /var/lib/rancher/k3s/agent/images/
   sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/
   ```


3. 将 k3s 二进制文件放在 `/usr/local/bin/k3s`路径下，并确保拥有可执行权限。完成后，现在可以转到下面的[安装 K3s](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#安装-k3s)部分，开始安装 K3s。



### 前提条件

+ 在安装 K3s 之前，完成上面的[部署私有镜像仓库](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#部署私有镜像仓库)或[手动部署镜像](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#手动部署镜像)，导入安装 K3s 所需要的镜像。
+ 从 [release](https://github.com/rancher/k3s/releases) 页面下载 K3s 二进制文件，K3s 二进制文件需要与离线镜像的版本匹配。将二进制文件放在每个离线节点的 `/usr/local/bin` 中，并确保这个二进制文件是可执行的。
+ 下载 K3s 安装脚本：[https://get.k3s.io](https://get.k3s.io/) 。将安装脚本放在每个离线节点的任意地方，并命名为 `install.sh`。

当使用 `INSTALL_K3S_SKIP_DOWNLOAD` 环境变量运行 K3s 脚本时，K3s 将使用本地的脚本和二进制。



::: warning 提醒 u
您可以在离线环境中执行单节点安装，在一个 server（节点）上安装 K3s，或高可用安装，在多个 server（节点）上安装 K3s。

对安装脚本进行简单的修改（ghproxy），在最后可以看到 安装脚本~

:::



### Containerd + 手动部署镜像方式

::: details 展开查看步骤
假设你已经将同一版本的 K3s 的安装脚本(`k3s-install.sh`)、K3s 的二进制文件(`k3s`)、K3s 依赖的镜像(`k3s-airgap-images-amd64.tar`)下载到了`/root`目录下。

如果你使用的容器运行时为containerd，在启动 K3s 时，它会检查`/var/lib/rancher/k3s/agent/images/`是否存在可用的镜像压缩包，如果存在，就将该镜像导入到 `containerd` 镜像列表中。所以我们只需要下载 `K3s` 依赖的镜像到`/var/lib/rancher/k3s/agent/images/`目录，然后启动 `K3s` 即可。



**1. 导入镜像到 `containerd` 的镜像列表：**

```bash
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp /root/k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
```



**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```bash
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/
```



**3. 安装 K3s**

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh
```

:::



::: details 演示

```bash
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# cp k3s-install.sh /root/k3s-install.sh
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# ls 
images  k3s  k3s-install.sh  Kubefile  sealer-runtime-demo
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# cp k3s  /root/k3s
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# sudo chmod a+x /root/k3s /root/k3s-install.sh
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# sudo cp /root/k3s /usr/local/bin/
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh
[INFO]  Skipping k3s download and verify
[INFO]  Skipping installation of SELinux RPM
[INFO]  Skipping /usr/local/bin/kubectl symlink to k3s, command exists in PATH at /usr/bin/kubectl
[INFO]  Skipping /usr/local/bin/crictl symlink to k3s, command exists in PATH at /usr/bin/crictl
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink from /etc/systemd/system/multi-user.target.wants/k3s.service to /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
Failed to restart k3s.service: Unit is not loaded properly: Invalid argument.
See system logs and 'systemctl status k3s.service' for details.
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# k3s
NAME:
   k3s - Kubernetes, but small and simple

USAGE:
   k3s [global options] command [command options] [arguments...]

VERSION:
   v1.25.3+k3s1 (f2585c16)

COMMANDS:
   server           Run management server
   agent            Run node agent
   kubectl          Run kubectl
   crictl           Run crictl
   ctr              Run ctr
   check-config     Run config check
   etcd-snapshot    Trigger an immediate etcd snapshot
   secrets-encrypt  Control secrets encryption and keys rotation
   certificate      Certificates management
   completion       Install shell completion script
   help, h          Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                     (logging) Turn on debug logs [$K3S_DEBUG]
   --data-dir value, -d value  (data) Folder to hold state (default: /var/lib/rancher/k3s or ${HOME}/.rancher/k3s if not root)
   --help, -h                  show help
   --version, -v               print the version

```

**验证：**

```bash
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# crictl images
WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
IMAGE                                    TAG                 IMAGE ID            SIZE
k0sproject/k0s                           latest              6adc65a599f7a       253MB
nginx                                    latest              76c69feac34e8       142MB
registry                                 2.7.1               0d0107588605f       25.7MB
sea.hub:5000/calico/apiserver            v3.22.1             b7dd079a4ed76       129MB
sea.hub:5000/calico/cni                  v3.22.1             2a8ef6985a3e5       236MB
sea.hub:5000/calico/kube-controllers     v3.22.1             c0c6672a66a59       132MB
sea.hub:5000/calico/node                 v3.22.1             7a71aca7b60fc       198MB
sea.hub:5000/calico/pod2daemon-flexvol   v3.22.1             17300d20daf93       19.7MB
sea.hub:5000/calico/typha                v3.22.1             f822f80398b9a       127MB
sea.hub:5000/coredns                     1.7.0               bfe3a36ebd252       45.2MB
sea.hub:5000/etcd                        3.4.13-0            0369cf4303ffd       253MB
sea.hub:5000/kube-apiserver              v1.19.8             9ba91a90b7d1b       119MB
sea.hub:5000/kube-controller-manager     v1.19.8             213ae7795128d       111MB
sea.hub:5000/kube-proxy                  v1.19.8             ea03182b84a23       118MB
sea.hub:5000/kube-scheduler              v1.19.8             919a3f36437dc       46.5MB
sea.hub:5000/pause                       3.2                 80d28bedfe5de       683kB
sea.hub:5000/tigera/operator             v1.25.3             648350e58702c       128MB
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]# kubectl get pods -A
NAMESPACE          NAME                                              READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-64f668766b-dv2xk                 1/1     Running   2          4d17h
calico-apiserver   calico-apiserver-64f668766b-k49gx                 1/1     Running   2          4d17h
calico-system      calico-kube-controllers-69dfd59986-mq7cv          1/1     Running   0          4d17h
calico-system      calico-node-pg47k                                 1/1     Running   0          4d17h
calico-system      calico-typha-84f56b949f-t95jk                     1/1     Running   0          4d17h
default            myapp                                             0/3     Pending   0          4d14h
kube-system        coredns-55bcc669d7-74xb2                          1/1     Running   0          4d17h
kube-system        coredns-55bcc669d7-jdkj2                          1/1     Running   0          4d17h
kube-system        etcd-izbp1evo5cnwagauz3w188z                      1/1     Running   0          4d17h
kube-system        kube-apiserver-izbp1evo5cnwagauz3w188z            1/1     Running   0          4d17h
kube-system        kube-controller-manager-izbp1evo5cnwagauz3w188z   1/1     Running   0          4d17h
kube-system        kube-proxy-ssr6t                                  1/1     Running   0          4d17h
kube-system        kube-scheduler-izbp1evo5cnwagauz3w188z            1/1     Running   0          4d17h
tigera-operator    tigera-operator-7cdb76dd8b-ltbbs                  1/1     Running   10         4d17h
```

:::





### Docker + 手动部署镜像方式

::: details 展开查看步骤
假设你已经将同一版本的 K3s 的安装脚本(`k3s-install.sh`)、K3s 的二进制文件(`k3s`)、K3s 依赖的镜像(`k3s-airgap-images-amd64.tar`)下载到了`/root`目录下。

与 `containerd` 不同，使用 docker 作为容器运行时，启动 `K3s` 不会导入 `/var/lib/rancher/k3s/agent/images/`目录下的镜像。所以在启动 `K3s` 之前我们需要将 `K3s` 依赖的镜像手动导入到 `docker` 镜像列表中。



**1. 导入镜像到 `docker` 的镜像列表：**

```bash
sudo docker load -i /root/k3s-airgap-images-amd64.tar
```



**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```bash
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/
```



**3. 安装 K3s**

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='--docker' /root/k3s-install.sh
```

:::





### Containerd + 手动部署镜像方式

::: details 展开查看步骤
假设你已经将同一版本的 K3s 的安装脚本(`k3s-install.sh`)、K3s 的二进制文件(`k3s`)、K3s 依赖的镜像(`k3s-airgap-images-amd64.tar`)下载到了`/root`目录下。

如果你使用的容器运行时为containerd，在启动 K3s 时，它会检查`/var/lib/rancher/k3s/agent/images/`是否存在可用的镜像压缩包，如果存在，就将该镜像导入到 `containerd` 镜像列表中。所以我们只需要下载 `K3s` 依赖的镜像到`/var/lib/rancher/k3s/agent/images/`目录，然后启动 `K3s` 即可。



**1. 导入镜像到 `containerd` 的镜像列表：**

```bash
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp /root/k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
```



**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```bash
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/
```



**3. 安装 K3s**

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh
```

:::



### Containerd + 私有镜像仓库方式

::: details 展开查看详细
假设你已经将同一版本的 K3s 的安装脚本(`k3s-install.sh`)、K3s 的二进制文件(k3s)下载到了`/root`目录下。并且 `K3s` 所需要的镜像已经上传到了镜像仓库（本例的镜像仓库地址为：http://192.168.64.44:5000）。K3s 所需的镜像列表可以从 `K3s Release`页面的`k3s-images.txt`获得。

**1. 配置 K3s 镜像仓库**

启动 K3s 默认会从docker.io拉取镜像。使用containerd容器运行时在离线安装时，我们只需要将镜像仓库地址配置到docker.io下的endpoint即可，更多配置说明请参考配置 containerd 镜像仓库完全攻略或[K3s 官方文档](https://docs.rancher.cn/docs/k3s/installation/private-registry/_index/)：

```bash
sudo mkdir -p /etc/rancher/k3s
sudo cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
"docker.io":
endpoint:
- "http://192.168.64.44:5000"
- "https://registry-1.docker.io"
EOF
```



**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```bash
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/
```



**3. 安装 K3s**

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh
```

> 稍等片刻，即可查看到 K3s 已经成功启动：

:::



### Docker + 私有镜像仓库方式

::: details 展开查看详细
假设你已经将同一版本的 K3s 的安装脚本(k3s-install.sh)、K3s 的二进制文件(k3s)下载到了/root目录下。并且 K3s 所需要的镜像已经上传到了镜像仓库（本例的镜像仓库地址为：http://192.168.64.44:5000）。K3s 所需的镜像列表可以从 K3s Release页面的k3s-images.txt获得。

**1. 配置 K3s 镜像仓库**

Docker 不支持像 containerd 那样可以通过修改 docker.io 对应的 endpoint（默认为 https://registry-1.docker.io）来间接修改默认镜像仓库的地址。但在Docker中可以通过配置registry-mirrors来实现从其他镜像仓库中获取K3s镜像。这样配置之后，会先从registry-mirrors配置的地址拉取镜像，如果获取不到才会从默认的docker.io获取镜像，从而满足了我们的需求。

```bash
cat >> /etc/docker/daemon.json <<EOF
{
"registry-mirrors": ["http://192.168.64.44:5000"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```



**2、将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```bash
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/
```



**3. 安装k3s：**

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='--docker' /root/k3s-install.sh
```

:::





### 单结点高可用离线安装

**提供要从 server 节点卸载 K3s，和需要从agent结点卸载K3s，推荐使用高可用安装，关于单结点迁移到高可用状态可参考 [🧷 这篇文章](https://mp.weixin.qq.com/s/Yax2m2uFw2d4lo5sybHsCw)：**

:::: code-group
::: code-group-item 单结点安装

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh
```

然后，要选择添加其他 agent，请在每个 agent 节点上执行以下操作。注意将 `myserver` 替换为 server 的 IP 或有效的 DNS，并将 `mynodetoken` 替换 server 节点的 token，通常在`/var/lib/rancher/k3s/server/node-token`。

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./install.sh
```

:::
::: code-group-item 高可用安装

```bash
curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint='mysql://username:password@tcp(hostname:3306)/database-name'
```

您需要调整安装命令，以便指定`INSTALL_K3S_SKIP_DOWNLOAD=true`并在本地运行安装脚本。您还将利用`INSTALL_K3S_EXEC='args'`为 k3s 提供其他参数。

由于在离线环境中无法使用`curl`命令进行安装，所以您需要参考以下示例，将这条命令行修改为离线安装：

```bash
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server' K3S_DATASTORE_ENDPOINT='mysql://username:password@tcp(hostname:3306)/database-name' ./install.sh
```

:::
::::



## 为kubelet 设置别名

`k3s` 安装之后内置了一个 `kubectl` 的子命令，我们通过执行 `k3s kubectl` 命令来调用它，其功能和使用方式都和 `k8s` 的 `kubectl` 命令是一致。为了我们更加方便的使用，可以设置一个 `alias` 别名或者创建一个软连接达到命令的无缝使用。

```bash
# 创建alias别名
alias kubectl='k3s kubectl'

# 创建软连接
ln -sf /usr/bin/kubectl /usr/local/bin/k3s

# 配置kubectl命令补全
source <(kubectl completion bash)
```

配置完成之后，就可以使用 `kubectl` 来操作集群机器了。通过运行如下命令，可以查看 `kube-system` 名称空间中运行的 `pod` 列表。我们发现并没有运行 `apiserver`、`scheduler`、`kube-proxy` 以及 `flannel` 等组件，因为这些都已经内嵌到了 `k3s` 进程中了。另外 `k3s` 已经给我们默认部署运行了 `traefik ingress`、`metrics-server` 等服务，不需要再额外安装了。

::: tip 一个很不成熟的想法
或许你也可以将 docker 作为 CRI ， 我不建议那么做，或许有一个折中的方法（也是设置别名）

```bash
# 创建alias别名
alias docker='k3s crictl'
```

:::







## 扩展work节点

K3s 提供了一个安装脚本，可以方便地将其作为服务安装在基于 systemd 或 openrc 的系统上。该脚本可在 [https://get.k3s.io](https://get.k3s.io/) 获得。要使用这种方法安装 K3s，只需运行：

```bash
curl -sfL https://get.k3s.io | sh -
```

运行此安装后：

+ K3s 服务将被配置为在节点重启后或进程崩溃或被杀死时自动重启。
+ 将安装其他实用程序，包括 `kubectl`、`crictl`、`ctr`、`k3s-killall.sh` 和 `k3s-uninstall.sh`。
+ [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) 文件将写入到 `/etc/rancher/k3s/k3s.yaml`，由 K3s 安装的 kubectl 将自动使用该文件。

要在 Worker 节点上安装并将它们添加到集群，请使用 `K3S_URL` 和 `K3S_TOKEN` 环境变量运行安装脚本。以下示例演示了如何加入 Worker 节点：

```bash
curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -
```

设置 `K3S_URL` 参数会使 K3s 以 Worker 模式运行。K3s Agent 将注册到在 URL 上监听的 K3s Server。`K3S_TOKEN` 使用的值存储在 Server 节点上的 `/var/lib/rancher/k3s/server/node-token` 中。

注意：每台主机必须具有唯一的主机名。如果你的计算机没有唯一的主机名，请传递 `K3S_NODE_NAME` 环境变量，并为每个节点提供一个有效且唯一的主机名。



## CRI CNI  CSI

**网络**：

+  因为 `k3s` 已经内置了 `Traefik` 组件，不需要再单独安装 `ingress controller` 了，直接创建 `Ingress` 即可。其中 `192.168.xxx.xxx` 为 `master` 节点的 `IP`，由于我们没有 `DNS` 解析，因此可以通过配置 `/etc/hosts` 文件进行静态配置，之后就可以通过域名来访问我们的服务了。

**网络：**

+  因为 `k3s` 已经内置了 `Flannel` 网络插件，默认使用 `VXLAN` 后端，默认 `IP` 段为 `10.42.0.0/16`。内置的 `Flannel` 除了 `VXLAN` 还支持 `ipsec`、`host-gw` 以及 `wireguard`。当然除了默认的 `Flannel`，`k3s` 还支持其他 `CNI`，如 `Canal`、`Calico` 等。

**存储：**

+  `k3s` 删除了 `k8s` 内置 `cloud provider` 以及 `storage` 插件，内置了 `Local Path Provider` 来提供存储。而内置 `local path` 存储，只能单机使用，不支持跨主机使用，也不支持存储的高可用。可以通过使用外部的存储插件解决 k3s 存储问题，比如 `Longhorn` 云原生分布式块存储系统。



## 嵌入式数据库高可用

> 在 K3s v1.19.1 中，嵌入式 etcd 取代了实验性的 `Dqlite`。这是一个突破性的变化。请注意，不支持从实验性 Dqlite 升级到嵌入式 etcd。如果你尝试升级，升级将不会成功，并且数据将会丢失。

etcd 使用的共识算法是 `raft`，HA模式下保证三个node开始~

首先，启动一个带有 `cluster-init` 标志的 Server 节点来启用集群和一个令牌，该令牌将作为共享 secret，用于将其他服务器加入集群。

```bash
curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init
```



启动第一台服务器后，使用共享 secret 将第二台和第三台服务器加入集群：

```bash
curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://<ip or hostname of server1>:6443
```



## HA 部署实验

+ `192.168.71.130`  server node
+ `192.168.71.131`   agent node
+ `192.168.71.132`   agent node

**环境：**

```bash
root@cubmaster01:/workspces# uname -a
Linux cubmaster01 5.4.0-132-generic #148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

root@etcnode01:/current# uname -a
Linux etcnode01 5.4.0-132-generic #148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

root@cubnode02:/tmp# uname -a
Linux cubnode02 5.4.0-132-generic #148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
```

**master节点(192.168.71.130)：**

> 如果网络原因，可以直接复制脚本在本地 `install.sh` 文件，然后运行：
>
> ```bash
> # 替换文件中的 https://github.com --> https://ghproxy.com/https://github.com
> INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubmaster01     K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config     INSTALL_K3S_EXEC="--docker" | sh install.sh 
> ```

```bash
# 指定了 kubeconfig 的位置
curl -sfL https://get.k3s.io  | \
    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubmaster01 \
    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \
    INSTALL_K3S_EXEC="--docker" sh -
```

::: details server 详细安装解释

```bash
# 查找stable分支版本信息
[INFO]  Finding release for channel stable
[INFO]  Using v1.23.6+k3s1 as release

# 获取国内镜像版本地址
[INFO]  Downloading hash https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s
[INFO]  Verifying binary download

# 安装k3s二进制工具并链接相关工具(内置)
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr

# 安装清除和卸载k3s生成的配置和工具
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh

# 常见了两个systemd的配置
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.

# 启动k3s服务
[INFO]  systemd: Starting k3s
```

:::



**node1节点(192.168.71.131)：**

+ [注册 node 节点](https://docs.rancher.cn/docs/k3s/architecture/_index/#%E6%B3%A8%E5%86%8C-agent-%E8%8A%82%E7%82%B9)

> **查看server的token：**
>
> ```bash
> # mynodetoken
> sudo cat /var/lib/rancher/k3s/server/token
> ```
>
> **本地加速：**
>
> ```sh
> INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubnode01     K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config     K3S_URL=https://192.168.71.130:6443     K3S_TOKEN=K100be446b6ae6fb8f9a551061516fcc2dac8196de80e747026626b9baab34d57be::server:6cf0d1c8342f340cf5e8344e19493cb7  sh install.sh
> ```

```bash
# 建议使用这个安装命令(国内化了)
# K3S_URL: 会使K3s以worker模式运行
# K3S_TOKEN: 使用的值存储在你的服务器节点上
# K3S_NODE_NAME: 为每个节点提供一个有效且唯一的主机名
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubnode01 \
    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \
    K3S_URL=https://192.168.71.130:6443 \
    K3S_TOKEN=mynodetoken sh -
```



::: details agent 节点安装详细说明

```bash
# 查找stable分支版本信息
[INFO]  Finding release for channel stable
[INFO]  Using v1.23.6+k3s1 as release

# 获取国内镜像版本地址
[INFO]  Downloading hash https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s
[INFO]  Verifying binary download

# 安装k3s二进制工具并链接相关工具(内置)
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s

# 安装清除和卸载k3s生成的配置和工具
[INFO]  Creating killall script /usr/local/bin/k3s-agent-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh

# 常见了两个systemd的配置
[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service
[INFO]  systemd: Enabling k3s-agent unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.

# 启动k3s服务
[INFO]  systemd: Starting k3s-agent
```

:::



**node2节点：**

> node1节点类似操作





## 卸载k3s

**卸载k3s：**

::: details 卸载k3s
如果您使用安装脚本安装了 K3s，那么在安装过程中会生成一个卸载 K3s 的脚本。

> 卸载 K3s 会删除集群数据和所有脚本。要使用不同的安装选项重新启动集群，请使用不同的标志重新运行安装脚本。

:::

提供要从 server 节点卸载 K3s，和需要从agent结点卸载K3s：

:::: code-group
::: code-group-item server结点

```bash
/usr/local/bin/k3s-uninstall.sh
```

:::
::: code-group-item agent结点

```bash
/usr/local/bin/k3s-agent-uninstall.sh
```

:::
::::

**优秀的你肯定会将他清除干净的：**

```bash
# 清除垃圾文件
rm -rf /etc/rancher /var/lib/rancher
```



###  针对 docker CRI

::: details 括docker等信息一并清理

```bash
# 包括docker等信息一并清理
#!/bin/bash

KUBE_SVC='
kubelet
kube-scheduler
kube-proxy
kube-controller-manager
kube-apiserver
'

for kube_svc in ${KUBE_SVC};
do
  # 停止服务
  if [[ `systemctl is-active ${kube_svc}` == 'active' ]]; then
    systemctl stop ${kube_svc}
  fi
  # 禁止服务开机启动
  if [[ `systemctl is-enabled ${kube_svc}` == 'enabled' ]]; then
    systemctl disable ${kube_svc}
  fi
done

# 停止所有容器
docker stop $(docker ps -aq)

# 删除所有容器
docker rm -f $(docker ps -qa)

# 删除所有容器卷
docker volume rm $(docker volume ls -q)

# 卸载mount目录
for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher;
do
  umount $mount;
done

# 备份目录
mv /etc/kubernetes /etc/kubernetes-bak-$(date +"%Y%m%d%H%M")
mv /var/lib/etcd /var/lib/etcd-bak-$(date +"%Y%m%d%H%M")
mv /var/lib/rancher /var/lib/rancher-bak-$(date +"%Y%m%d%H%M")
mv /opt/rke /opt/rke-bak-$(date +"%Y%m%d%H%M")

# 删除残留路径
rm -rf /etc/ceph \
    /etc/cni \
    /opt/cni \
    /run/secrets/kubernetes.io \
    /run/calico \
    /run/flannel \
    /var/lib/calico \
    /var/lib/cni \
    /var/lib/kubelet \
    /var/log/containers \
    /var/log/kube-audit \
    /var/log/pods \
    /var/run/calico \
    /usr/libexec/kubernetes

# 清理网络接口
no_del_net_inter='
lo
docker0
eth
ens
bond
'

network_interface=`ls /sys/class/net`

for net_inter in $network_interface;
do
  if ! echo "${no_del_net_inter}" | grep -qE ${net_inter:0:3}; then
    ip link delete $net_inter
  fi
done

# 清理残留进程
port_list='
80
443
6443
2376
2379
2380
8472
9099
10250
10254
'

for port in $port_list;
do
  pid=`netstat -atlnup | grep $port | awk '{print $7}' | awk -F '/' '{print $1}' | grep -v - | sort -rnk2 | uniq`
  if [[ -n $pid ]]; then
    kill -9 $pid
  fi
done

kube_pid=`ps -ef | grep -v grep | grep kube | awk '{print $2}'`

if [[ -n $kube_pid ]]; then
  kill -9 $kube_pid
fi

# 清理Iptables表
## 注意：如果节点Iptables有特殊配置，以下命令请谨慎操作
sudo iptables --flush
sudo iptables --flush --table nat
sudo iptables --flush --table filter
sudo iptables --table nat --delete-chain
sudo iptables --table filter --delete-chain
systemctl restart docker
```

:::



## k3s 的一些重要目录

k3s是一种轻量级的Kubernetes发行版，它在安装时会创建一些重要的目录，具体如下：

+ /etc/rancher/k3s/：包含k3s配置文件和证书。
+ /var/lib/rancher/k3s/：包含k3s数据，例如etcd数据库、服务注册表、DNS缓存和节点信息。
+ /var/log/rancher/k3s/：包含k3s服务的日志文件。
+ /usr/local/bin/：包含k3s的可执行文件，例如kubectl和k3s服务管理工具。

这些目录的位置可能会根据您使用的安装方式而有所不同。例如，如果您使用的是内置的SQLite数据库，那么可能会将数据存储在 `/var/lib/rancher/k3s/data/` 目录下，而不是独立的etcd数据库。



::: tip 
您可以将主要的 k3s 二进制文件放在任何您想要的地方。它会将内容写入 `/var/lib/rancher/k3s` 和 `/etc/rancher`，以及 `containerd` 和 `kubelet` 用于非持久文件的正常位置 `/var/run` 下。

可以在 `/var/lib/rancher/k3s` 路径中找到 `db` 目录（SQLite）。

:::

`/usr/local/bin` 目录中有 k3s 的一些关键目录：

```bash
root@etcnode01:/usr/local/bin# ls -al
total 66164
drwxr-xr-x  2 root root     4096 Nov 25 04:31 .
drwxr-xr-x 10 root root     4096 Aug 31 06:52 ..
lrwxrwxrwx  1 root root        3 Nov 25 04:31 crictl -> k3s
-rwxr-xr-x  1 root root 67735552 Nov 25 04:31 k3s
-rwxr-xr-x  1 root root     1433 Nov 25 04:31 k3s-agent-uninstall.sh
-rwxr-xr-x  1 root root     2026 Nov 25 04:31 k3s-killall.sh
lrwxrwxrwx  1 root root        3 Nov 25 04:31 kubectl -> k3s
```

Linux 系统下提供 ln 指令来进行文件链接

我们可以看到这里面有很多的连接文件，同样的，其实在 k3s 的 rootfs bin目录下也是这样的

::: detatils 软连接 & 硬连接
`ln` 指令默认创建的是硬链接，如果加入了`-s`参数，则会生成一个软链接。

**硬连接：**

在 Linux 中，多个文件名指向同一索引节点是存在的，所以硬连接指通过索引节点来进行的连接，即每一个硬链接都是一个指向对应区域的文件。

硬链接的作用是允许一个文件**拥有多个有效路径名**，这样用户就可以建立硬链接到重要文件,以防止“误删”的功能。

只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个链接被删除后，文件的数据块及目录的连接才会被释放，也就是说，文件才会被真正删除。

⚠️ 注意这并不是 `cp` ，这不是重复文件，注意！！！它们只是指向同一个文件的索引。



**软连接：**

![image-20221125211941734](http://sm.nsddd.top/smimage-20221125211941734.png)

软链接又叫符号链接，这个文件包含了另一个文件的路径名，例如在上图中，`foo.txt` 就是 `bar.txt` 的软连接，`bar.txt` 是实际的文件，`foo.txt`包含的是对于 `bar.txt` 的 inode 的记录。

软连接可以是任意文件或目录，可以链接不同文件系统的文件，在对符号文件进行读或写操作的时候，系统会自动把该操作转换为对源文件的操作，但删除链接文件时，系统仅仅删除链接文件，而不删除源文件本身，这一点类似于 Windows 操作系统下的快捷方式。

> 软链接以 `l` 开头：
>
> ```bash
> lrwxrwxrwx  1 root root    1 Nov 25 13:21 x1 -> x
> ```

:::



### `/var/lib/rancher/k3s`

```
[root@VM-4-6-centos k3s]# pwd;ls
/var/lib/rancher/k3s
agent  data  server
```

::: warning 目录解释
**agent中：**

```bash
root@cubmaster01:/var/lib/rancher/k3s/agent# ls
client-ca.crt              client-kube-proxy.key     pod-manifests
client-k3s-controller.crt  containerd                server-ca.crt
client-k3s-controller.key  etc                       serving-kubelet.crt
client-kubelet.crt         k3scontroller.kubeconfig  serving-kubelet.key
client-kubelet.key         kubelet.kubeconfig
client-kube-proxy.crt      kubeproxy.kubeconfig
```



**data中：**

运行的工作目录：

```bash
root@cubmaster01:/var/lib/rancher/k3s/data# ls
7c994f47fd344e1637da337b92c51433c255b387d207b30b3e0262779457afe4
current
```



**server中：**

+ `manifests`：位于目录路径`/var/lib/rancher/k3s/server/manifests` 的清单在构建时被捆绑到 K3s 二进制文件中，将由[rancher/helm-controller](https://github.com/rancher/helm-controller#helm-controller)在运行时安装。

  ```bash
  root@ubuntu:/var/lib/rancher/k3s/server# cd manifests/;ls;pwd
  ccm.yaml      local-storage.yaml  rolebindings.yaml
  coredns.yaml  metrics-server      traefik.yaml
  /var/lib/rancher/k3s/server/manifests
  ```

+ `tls`：tls证书，k3s证书默认是一年

+ `db`：数据库

+ `token、node-token、agent-token`：token

+ 主节点的密码存储：`/var/lib/rancher/k3s/server/cred/node-passwd`

:::



### `/etc/rancher`

```
[root@VM-4-6-centos rancher]# pwd;ls
/etc/rancher
k3s  node
```

::: warning 目录解释
**k3s中：**

+ k3s.yaml：k3s 集群的一些元数据

+ registries.yaml：注册表一些信息，镜像加速

**node：**

+ `password`：Agent 将使用节点集群 secret 以及随机生成的节点密码向 k3s server 注册存储密码路径

> K3s server 将把各个节点的密码存储为 Kubernetes secrets，随后的任何尝试都必须使用相同的密码。节点密码秘密存储在`kube-system`命名空间中，名称使用模板`<host>.node-password.k3s`

:::



### `/var/run`

+ `k3s`：CRI（containerd or docker)

  ```
  root@cubmaster01:/var/run/k3s/containerd# ls
  containerd.sock            io.containerd.runtime.v1.linux
  containerd.sock.ttrpc      io.containerd.runtime.v2.task
  io.containerd.grpc.v1.cri
  ```

+ `containerd`

  ```bash
  root@cubmaster01:/var/run/containerd# ls
  containerd.sock        io.containerd.runtime.v1.linux  runc
  containerd.sock.ttrpc  io.containerd.runtime.v2.task   s
  ```



## 镜像加速

镜像加速配置后，重启服务

```bash
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"
EOF
```

重启k3s使配置生效

```bash
# 重启服务
sudo systemctl restart k3s

# 是否生效
sudo crictl info | grep -A 2 "endpoint"

crictl info|grep  -A 5 registry
```

![image-20221031112848849](http://sm.nsddd.top/smimage-20221031112848849.png)





## containerd

+ [https://containerd.io/](https://containerd.io/)

### 架构图

![image-20221110202936935](http://sm.nsddd.top/smimage-20221110202936935.png)

::: details 补充containerd
containerd从docker就开始熟悉的，那么自然从docker开始介绍：

![img](https://sm.nsddd.top/sm952033-20180520115357747-1796034956.png)



> 在docker1.8之前可以使用 `docker -d`。在后面就是 `docker daemon` 。1.11以后：`docker`、`dockerd`。2015年后 OCI 成立，`runtime-spec` 制定
>
> `libcotainer –>  runC`
>
> ```bash
> dockerd = docker engine + containerd + containerd - shim + runC
> ```
>
> …….
>
> 后面 `kubelet` 不支持 `docker` （因为 `docker` 不支持 `CRI`），`kubernetes`使用 `containerd`。`containerd v1.1`后面也支持 `cri` ，

从图中可以看出，docker 对容器的管理和操作基本都是通过 containerd 完成的。 那么，containerd 是什么呢？

> **containerd** 可用作 Linux 和 Windows 的守护进程。它管理其主机系统的整个容器生命周期，从映像传输和存储到容器执行和监督，再到低级存储，再到网络附件等。

**Containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。Containerd 可以在宿主机中管理完整的容器生命周期：容器镜像的传输和存储、容器的执行和管理、存储和网络等。** 详细点说，Containerd 负责干下面这些事情：

+ 管理容器的生命周期(从创建容器到销毁容器)
+ 拉取 / 推送容器镜像
+ 存储管理(管理镜像及容器数据的存储)
+ 调用 `runC` 运行容器(与 `runC` 等容器运行时交互)
+ 管理容器网络接口及网络

⚠️ 注意：**Containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用。**

![image-20221031142456840](http://sm.nsddd.top/smimage-20221031142456840.png)

:::



::: tip 
在上面的安装我们知道了可以选择默认的docker安装。

:::

###  命令

![查看源图像](http://sm.nsddd.top/smcontainerd-docker-k8s-images)





### containerd的配置管理

::: warning 总结
k3s 安装后内置以下 containerd 客户端

+ `ctr` ： 单纯的容器管理
+ `crictl`：从 kubernetes 视角触发，对 POD，容器进行管理。

**k3s 内修改 containerd 的配置步骤：**

+ 复制 `/var/lib/rancher/k3s/agent/containerd/config.toml` 为同目录下的新模板
+ 修改 `config.toml.tmpl`
+ 重启 `k3s` （systemctl restart k3s) 或者 `k3s-agent`（systemctl restart k3s-agent）
+ 检查 `/var/lib/rancher/k3s/agent/containerd/config.toml` 

:::



**日志：**

```bash
tail -f /var/lib/rancher/k3s/agent/containerd/containerd.log
```



## 二进制工具

K3s 二进制文件包含许多帮助您管理集群的附加工具。

| 命令                  | 描述                                                         |
| --------------------- | ------------------------------------------------------------ |
| `k3s server`          | 运行 K3s 管理服务器，它还将启动 Kubernetes 控制平面组件，例如 API 服务器、控制器管理器和调度程序。 |
| `k3s agent`           | 运行 K3s 节点代理。这将导致 K3s 作为工作节点运行，启动 Kubernetes 节点服务`kubelet`和`kube-proxy`. |
| `k3s kubectl`         | 运行嵌入式[kubectl](https://kubernetes.io/docs/docs/reference/kubectl/overview/) CLI。如果`KUBECONFIG`未设置环境变量，这将自动尝试使用在`/etc/rancher/k3s/k3s.yaml`启动 K3s 服务器节点时创建的配置文件。 |
| `k3s crictl`          | 运行嵌入式[crictl](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)。这是一个用于与 Kubernetes 的容器运行时接口 (CRI) 交互的 CLI。对调试很有用。 |
| `k3s ctr`             | 运行嵌入式[ctr](https://github.com/projectatomic/containerd/blob/master/docs/cli.md)。这是 containerd 的 CLI，K3s 使用的容器守护进程。对调试很有用。 |
| `k3s etcd-snapshot`   | 对 K3s 集群数据进行按需备份并上传到 S3。有关详细信息，请参阅[备份和还原](https://docs.k3s.io/backup-restore#backup-and-restore-with-embedded-etcd-datastore-experimental)。 |
| `k3s secrets-encrypt` | 将 K3s 配置为在将机密存储在集群中时对其进行加密。有关详细信息，请参阅[秘密加密](https://docs.k3s.io/security/secrets-encryption)。 |
| `k3s certificate`     | 证书管理                                                     |
| `k3s completion`      | 为 k3s 生成 shell 完成脚本                                   |
| `k3s help`            | 显示命令列表或一个命令的帮助                                 |



## 边缘计算

k3s 非常支持边缘计算，CICD 的部署，可以给我们带来更好的体验。

::: tip 边缘计算是什么？
边缘计算是为应用开发者和服务提供商在网络的边缘侧提供云服务和IT环境服务；目标是在靠近数据输入或用户的地方提供计算、存储和网络带宽。

通俗地说：边缘计算本质上是一种服务，就类似于云计算、大数据服务，但这种服务非常靠近用户；为什么要这么近？目的是为了让用户感觉到刷什么内容都特别快。

:::



**提升了Quick start成功率：**

我们在交付软件的时候，从以前的给一个Java环境到现在需要一个k8s 环境，k3s则集成了，提供开箱即用的交互体验，降低软件的资源占用，并且使运维部署更方便。



## 单节点 SQLite 扩展为 etcd 高可用

> 注意：k3s v1.22.2 及更新版本才支持从单节点 k3s 集群转换为内置 etcd 集群 

首先你需要有一个单节点的 k3s 集群，本例使用 1 master 节点、1 worker 节点的 k3s 集群。

```bash
root@cubmaster01:~/.kube# kubectl get node
NAME          STATUS   ROLES                  AGE    VERSION
cubnode01     Ready    <none>                 96m    v1.25.4+k3s1
cubmaster01   Ready    control-plane,master   142m   v1.25.4+k3s1
```

**验证集群：**

```bash
root@cubmaster01:~/.kube# kubectl get deployment,svc
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   145m
```



**将单节点 K3s 集群转换为内置 etcd 高可用集群**

首先，先停止 k3s 服务：

```bash
root@cubmaster01:~/.kube# systemctl stop k3s
```



通过使用 `--cluster-init` 标志重新启动你的 `k3s server` 来将其转换为 etcd 集群

```bash
root@cubmaster01:/workspces/runtime#  curl -sfL https://get.k3s.io | sh -s - --cluster-init
```



查看 master 节点的角色，来确认是否转换成功：

```bash
root@cubmaster01:/workspces/runtime# kubectl get nodes
NAME          STATUS   ROLES                       AGE    VERSION
cubmaster01   Ready    control-plane,etcd,master   151m   v1.25.4+k3s1
cubnode01     Ready    <none>                      105m   v1.25.4+k3s1
```

从上面 ROLES 列可以看到，master 节点的角色增加了 etcd，证明已经通过内置 etcd 数据库重新启动了 k3s 集群。



**验证集群：**

```
root@cubmaster01:/workspces/runtime# kubectl get deployment,svc
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   159m
```



**数据库文件：**

```
root@cubmaster01:/workspces/runtime# ls /var/lib/rancher/k3s/server/db/
etcd  state.db.migrated  state.db-shm  state.db-wal
```



## 安装脚本

::: details k3s 安装脚本
https://get.k3s.io 

Maybe you can try my plan, if you don't choose the domestic route, but you are affected by the firewall. Then you can use `https://ghproxy.com/{github-url}`

:::

::: details 国内镜像加速~
https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh

```
curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -
```

:::



### 理解安装的步骤

**先决条件：**

+ 选择上，两个节点不能有相同的主机名
+ 不修改主机名可以通过添加随机后缀或指定主机名



**硬件信息：**

+ 操作系统：可以在大多数现代 Linux 系统上运行（我希望可以有一个测试，我们不用再依赖于 kubernetes 的沉重标配了（不低于4 核）
+ 磁盘设备：K3s 的性能取决于数据库的性能(建议使用 SSD 硬盘)
+ 网络相关：K3s Server 节点的入站规则，所有出站流量都是允许的

| 协议 | 端口      | 源                       | 描述                         |
| :--- | :-------- | :----------------------- | :--------------------------- |
| TCP  | 6443      | K3s agent 节点           | Kubernetes API Server        |
| UDP  | 8472      | K3s server 和 agent 节点 | 仅对 Flannel VXLAN 需要      |
| TCP  | 10250     | K3s server 和 agent 节点 | Kubelet metrics              |
| TCP  | 2379-2380 | K3s server 节点          | 只有嵌入式 etcd 高可用才需要 |



**安装选项：**

+ [官方安装参数文档](https://docs.rancher.cn/docs/k3s/autok3s/_index/)
+ [安装选项示例演示](https://github.com/kingsd041/k3s-tutorial/blob/main/03-%E5%AE%89%E8%A3%85-%E8%A6%81%E6%B1%82%E5%8F%8A%E9%80%89%E9%A1%B9/README.md)

| Environment Variable        | Description                                        |
| :-------------------------- | :------------------------------------------------- |
| `INSTALL_K3S_EXEC`          | 用于在服务中启动 `K3s` 的后续子命令                |
| `K3S_CONFIG_FILE`           | 指定配置文件的位置                                 |
| `K3S_TOKEN`                 | 用于将 `server/agent` 加入集群的共享 `secret` 值   |
| `K3S_TOKEN_FILE`            | 用于将 `server/agent` 加入集群的共享 `secret` 文件 |
| `INSTALL_K3S_VERSION`       | 指定下载 `K3s` 的版本                              |
| `K3S_TOKEN_FILE`            | 指定  `cluster-secret`/`token` 的文件目录          |
| `INSTALL_K3S_SKIP_START`    | 将不会启动 `K3s` 服务                              |
| `INSTALL_K3S_SKIP_DOWNLOAD` | 用于离线安装；设置之后不会下载远程工具             |

```bash
# 其实就把对应参数加到systemd配置文件里面去了
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--docker" sh -

# 自动化部署(不用获取token值了)
# 主节点和工作节点使用我们指定的key来通信
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_TOKEN=rancher-k3s sh -
sudo cat /var/lib/rancher/k3s/server/token
```



**其他说明**

+ 运行 `agent` 时还必须设置 `K3S_TOKEN`
+ 以 `K3S_` 开头的环境变量将被保留，供 `systemd/openrc` 使用
+ 没有明确设置 `exec` 并设置 `K3S_URL` 的话会将命令默认为工作节点



### 标志和环境变量

在整个 k3s 文档学习中，会看到一些选项可以作为命令标志和环境变量传递进来，那该如何使用标志和环境变量呢？

```bash
# 使用标志
curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s -
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644

# 环境变量
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_EXEC="--flannel-backend none" sh -s -
curl -sfL https://get.k3s.io | \
    sh -s - server --flannel-backend none
```



### K3s Server/Agent - 常用配置

::: details 展开查看

```bash
# write-kubeconfig
# 将管理客户端的kubeconfig写入这个文件
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_KUBECONFIG_OUTPUT=/root/.kube/config \
    sh -

# 使用docker作为容器运行时
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--docker" sh -

# 指定运行时工具
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--container-runtime-endpoint containerd" \
    sh -


# 设置私有镜像仓库配置文件
# 默认配置文件: /etc/rancher/k3s/registries.yaml
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--private-registry xxx" \
    sh -

# 针对多网卡主机安装K3s集群
# 默认多网卡会使用默认网关的那个卡
route -n

# K3s server
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--node-ip=192.168.100.100" \
    sh -

# K3s agent  K3S_URL=https:?
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_URL=https://192.168.71.130:6443 K3S_TOKEN=xxx \
    INSTALL_K3S_EXEC="--node-ip=192.168.71.131" \
    sh -

# --tls-san
# 在TLS证书中添加其他主机名或IP作为主机备用名称
# 即在公网环境下允许通过公网IP访问控制、操作远程集群
# 或者部署多个Server并使用LB进行负责，就需要保留公网地址
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--tls-san 1.1.1.1"  \
    sh -

# 获取配置
kubectl get secret k3s-serving -n kube-system -o yaml

# 然后本机复制公网主节点对应的yaml文件即可本地操作了
scp ci@1.1.1.1:/etc/rancher/k3s/k3s.yaml ~/.kube/config

# 修改启动的服务对应配置(调整节点的启动的最大Pod数量)
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kubelet-arg=max-pods=200' \
    sh -

# 修改启动的服务对应配置(使用ipvs作为服务调度工具)
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kube-proxy-arg=proxy-mode=ipvs' \
    sh -

# 修改启动的服务对应配置(调整服务启动的端口范围)
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kube-apiserver-arg=service-node-port-range=40000-50000' \
    sh -

# kubelet-arg     --kubelet-arg
# kube-apiserver  --kube-apiserver-arg
# kube-proxy-arg  --kube-proxy-arg
# kube-proxy-arg  --kube-proxy-arg=proxy-mode=ipvs
# --data-dir
# 修改K3s数据存储目录
$ curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--data-dir=/opt/k3s-data' \
    sh -

# 禁用组件
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--disable traefik' \
    sh -

# 自己加自己需要的服务
ls /var/lib/rancher/k3s/server/manifests
kubectl get pods -A | grep traefik

# 添加label和taint标识
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--node-label foo=bar,hello=world \
        --node-taint key1=value1:NoExecute'
    sh -

# 查看一下
kubectl describe nodes
```

:::



### K3s Server/Agent - 数据库选项

```bash
# 指定数据源名称
# 标志位: --datastore-endpoint&nbsp;value
# 环境变量: K3S_DATASTORE_ENDPOINT
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--datastore-endpoint&nbsp;etcd' \
    sh -
    
 
# cron规范中的快照间隔时间
# --etcd-snapshot-schedule-cron&nbsp;value
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--etcd-snapshot-schedule-cron *&nbsp;*/5&nbsp;*&nbsp;*&nbsp;*' \
    sh -
```



### K3s 安装事项 - 网络选项

默认情况下，`K3s` 将以 `flannel` 作为 `CNI` 运行，使用 `VXLAN` 作为默认后端，`CNI` 和默认后端都可以通过参数修改。要启用加密，请使用下面的 `IPSec` 或 `WireGuard` 选项。

```bash
# 默认安装K3s之后的网络配置
sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "EnableIPv6": false,
    "EnableIPv4": true,
    "IPv6Network": "::/0",
    "Backend": {
        "Type": "vxlan"
    }
}
```

| CLI Flag 和 Value             | 描述                                      |
| :---------------------------- | :---------------------------------------- |
| `--flannel-backend=vxlan`     | 使用 `VXLAN` 后端(默认)                   |
| `--flannel-backend=host-gw`   | 使用 `host-gw` 后端                       |
| `--flannel-backend=ipsec`     | 使用 `IPSEC` 后端；对网络流量进行加密     |
| `--flannel-backend=wireguard` | 使用 `WireGuard` 后端；对网络流量进行加密 |



**配置 Flannel 选项**

这样，我就可以在安装 `K3s` 或者之后修改对应配置文件，来修改 `Flannel` 默认的后端网络配置选项(重启会覆盖不生效)了。下面，我们演示下，如何修改为 `host-gw` 模式。

```bash
# 主节点
# flannel-backend使用host-gw
# 该模式会把对端主机的IP当做默认网管(多Server情况)
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=host-gw' \
    sh -

# 工作节点
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.100.100:6443 \
    K3S_TOKEN=xxx sh -

# 默认的路由信息
route -n
0.0.0.0         172.16.64.1     0.0.0.0         UG    100    0        0 enp0s2
10.42.1.0       172.16.64.9     255.255.255.0   UG    0      0        0 enp0s2

# 查看配置之后的网络配置
sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "Backend": {
        "Type": "host-gw"
    }
}
```



**启用 Directrouting 特性**

**Flannel 自身的特性**：当主机在同一子网时，启用 `direct routes`(如 `host-gw`)。`vxlan` 只用于将数据包封装到不同子网的主机上，同子网的主机之间使用  `host-gw`，默认值为 `false`。

要添加我们就不能修改其对应的网络配置文件，因为重新安装或者重启都会把这个配置冲掉(变成默认配置)，所以需要折中下。我们自建一个网络配置文件，然后在启动的时候执行从哪个配置文件里面加载对应配置。

```bash
# k3s的master和agent
sudo cat /etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "Backend": {
        "Type": "vxlan",
        "Directrouting": true
    }
}

# k3s master
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=host-gw' \
    sh -
```



**自定义 CNI**

使用  `--flannel-backend=none`(禁用) 运行 `K3s`，然后在安装你选择的 `CNI`。按照 Calico CNI 插件指南 来修改 `Calico` 的 `YAML` 配置文件，在 `container_settings` 部分中允许 `IP` 转发。

```bash
# 加到Calico的YAML文件中
# 允许IP转发(这个是K3s的一个限制；需要开启)
"container_settings": {
    "allow_ip_forwarding": true
}

- name: CALICO_IPV4POOL_CIDR
  value: "192.168.200.0/24"

# 通过在主机上运行以下命令，确保设置已被应用(true)
sudo cat /etc/cni/net.d/10-canal.conflist

# calico
# 其中--cluster-cidr可不设置
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=none \
        --cluster-cidr=192.168.200.0/24"' \
    sh -

# 启动网络服务
kubectl apply -f ./calico.yaml
```



### 外部数据库

> **理解 Server 节点的安装，以及注册 Agent 节点的步骤！**

**使用外部数据库实现高可用安装**

+ 两个或多个`server` 节点
+ 零个或多个`agent` 节点
+ 外部数据存储(`Etcd/MySQL/PostgRES`)
+ 固定的注册地址(`LB`)
+ 这应该是最适合国内用户的 **K3s HA** 方案： https://mp.weixin.qq.com/s/0Wk2MzfWqMqt8DfUK_2ICA

虽然单节点 `k3s server` 集群可以满足各种用例，但是对于需要稳定运行的重要环境，可以在 `HA` 配置中运行 `K3s`，如何使用外部数据库安装一个高可用的 `K3s` 集群？

| 主机名       | 角色       | IP            |
| :----------- | :--------- | :------------ |
| k3s-server-1 | k3s master | 172.31.2.134  |
| k3s-server-2 | k3s master | 172.31.2.42   |
| k3s-db       | DB         | 172.31.10.251 |
| k3s-lb       | LB         | 172.31.13.97  |
| k3s-agent    | k3s agent  | 172.31.15.130 |



::: details 展开查看安装过程

```bash
# 1.创建一个外部数据存储
docker run --name some-mysql \
    --restart=unless-stopped -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD=password -d mysql:5.7

# 2.启动k3s-server节点(有读写权限不用加库名)
# mysql://username:password@tcp(hostname:3306)/database-name
# 可加污点 --node-taint CriticalAddonsOnly=true:NoExecute
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn sh - server \
    --datastore-endpoint="mysql://root:password@ip:3306/k3s" \
    --tls-san 172.31.13.97

# 3.配置固定的注册地址(k3s-lb节点)
# Agent节点需要一个URL来注册(LB)
cat >> /etc/nginx.conf <<EOF
worker_processes 4;
worker_rlimit_nofile 40000;

events {
    worker_connections 8192;
}

stream {
    upstream k3s_api {
        least_conn;
        server 172.31.2.134:6443 max_fails=3 fail_timeout=5s;
        server 172.31.2.42:6443 max_fails=3 fail_timeout=5s;
    }
    server {
        listen     6443;
        proxy_pass k3s_api;
    }
}
EOF

# 启动服务
docker run -d --restart=unless-stopped \
  -p 6443:6443 \
  -v /etc/nginx.conf:/etc/nginx/nginx.conf \
  nginx:1.14

# 4.加入Agent节点
# Agent会保存LB节点和每个Server节点的IP信息
# cat /var/lib/rancher/k3s/agent/etc/k3s-agent-load-balancer.json
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn
    K3S_URL=https://172.31.13.97:6443 K3S_TOKEN=mynodetoken \
    sh -

# 5.通过kubeconfig访问K3s集群
kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
k3s-server-1   Ready    control-plane,master   68s   v1.20.7+k3s1
k3s-server-2   Ready    control-plane,master   66s   v1.20.7+k3s1
```



**嵌入式 DB 的高可用**

要在这种模式下运行 `K3s`，你必须有**奇数的服务器节点**（raft)，建议从三个节点开始。在嵌入式中，默认使用 `Etcd` 作为高可用的数据库。

```bash
# 服务器节点(启动etcd集群)
# SECRET我们预定一个key值
# 使用cluster-init标志来启用集群
# 并使用一个标记作为共享的密钥来加入其他服务器到集群中
root@cubnode02:/workspces# curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \
    sh -s - --cluster-init

# 查看类型
root@cubnode02:/workspces# sudo  kubectl get nodes
NAME    STATUS  ROLES                      AGE  VERSION
ip-xxx  Ready   control-plane,etcd,master  19h  v1.23.6+k3s1

# 其他服务器节点(2/3)
root@cubnode02:/workspces# curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \
    sh -s - --server https://<ip-or-host-server>:6443

# 查询ETCD集群状态
# etcd证书默认目录：/var/lib/rancher/k3s/server/tls/etcd
# etcd数据默认目录：/var/lib/rancher/k3s/server/db/etcd
root@cubnode02:/workspces# ETCDCTL_ENDPOINTS='https://172.31.12.136:2379,\
    https://172.31.4.43:2379,\
    https://172.31.4.190:2379' \
ETCDCTL_CACERT='/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt' \
ETCDCTL_CERT='/var/lib/rancher/k3s/server/tls/etcd/server-client.crt'\
ETCDCTL_KEY='/var/lib/rancher/k3s/server/tls/etcd/server-client.key' \
ETCDCTL_API=3 etcdctl endpoint status --write-out=table
```



### 集群数据存储选项

使用 `etcd` 以外的数据存储运行 `K8S` 的能力使 `K3s` 区别于其他 `K8S` 发行版。该功能为 `K8S` 操作者提供了灵活性，可用的数据存储选项允许你选择一个最适合用例的数据存储。

如果你的团队没有操作 `etcd` 的专业知识，可以选择 `MySQL` 或 `PostgreSQL` 等企业级 `SQL` 数据库。如果您需要在 `CI/CD` 环境中运行一个简单的、短暂的集群，可以使用嵌入式 `SQLite` 数据库

如果你想使用外部数据存储，如 `PostgreSQL`、`MySQL` 或 `etcd`，你必须设置 `datastore-endpoint` 参数，以便 `K3s` 知道如何连接到它，也可以指定参数来配置连接的认证和加密。下表总结了这些参数，它们可以作为 `CLI` 标志或环境变量传递。

| CLI Flag               | 环境变量                 | 描述                                                         |
| :--------------------- | :----------------------- | :----------------------------------------------------------- |
| `--datastore-endpoint` | `K3S_DATASTORE_ENDPOINT` | 指定一个 PostgresSQL、MySQL 或 etcd 连接字符串。用于描述与数据存储的连接。这个字符串的结构是特定于每个后端的，详情如下。 |
| `--datastore-cafile`   | `K3S_DATASTORE_CAFILE`   | TLS 证书颁发机构（CA）文件，用于帮助确保与数据存储的通信安全。如果你的数据存储通过 TLS 服务请求，使用由自定义证书颁发机构签署的证书，你可以使用这个参数指定该 CA，这样 K3s 客户端就可以正确验证证书。 |
| `--datastore-certfile` | `K3S_DATASTORE_CERTFILE` | TLS 证书文件，用于对数据存储进行基于客户端证书的验证。要使用这个功能，你的数据存储必须被配置为支持基于客户端证书的认证。如果你指定了这个参数，你还必须指定`datastore-keyfile`参数。 |
| `--datastore-keyfile`  | `K3S_DATASTORE_KEYFILE`  | TLS 密钥文件，用于对数据存储进行基于客户端证书的认证。更多细节请参见前面的`datastore-certfile`参数。 |

作为最佳实践，我们建议将这些参数设置为环境变量，而不是命令行参数，这样你的数据库证书或其他敏感信息就不会作为进程信息的一部分暴露出来。



## 私有镜像仓库

`K3s` 默认使用 `containerd` 作为容器运行时，所以在 `docker` 上配置镜像仓库是不生效的。`K3s` 镜像仓库配置文件由两大部分组成：`mirrors` 和  `configs`。

+ `Mirrors` 是一个用于定义专用镜像仓库的名称和 `endpoint` 的指令
+ `Configs` 部分定义了每个 `mirror` 的 `TLS` 和证书配置
+ 对于每个 `mirror`，你可以定义 `auth` 和 `/` 或 `tls`

`K3s registry` 配置目录为： `/etc/rancher/k3s/registries.yaml`。`K3s` 启动时会检查 `/etc/rancher/k3s/` 中是否存在  `registries.yaml` 文件，并指示 `containerd` 使用文件中定义的镜像仓库。如果你想使用一个私有的镜像仓库，那么你需要在每个使用镜像仓库的节点上以 `root` 身份创建这个文件。

请注意，`server` 节点默认是可以调度的。如果你没有在 `server` 节点上设置污点，那么将在它们上运行工作负载，请确保在每个 `server` 节点上创建  `registries.yaml` 文件。

`containerd` 使用了类似 `K8S` 中 `svc` 与 `endpoint` 的概念，`svc` 可以理解为访问名称，这个名称会解析到对应的 `endpoint` 上。也可以理解 `mirror` 配置就是一个反向代理，它把客户端的请求代理到 `endpoint` 配置的后端镜像仓库。`mirror` 名称可以随意填写，但是必须符合 `IP` 或域名的定义规则。并且可以配置多个 `endpoint`，默认解析到第一个 `endpoint`，如果第一个 `endpoint` 没有返回数据，则自动切换到第二个 `endpoint`，以此类推。

```bash
# /etc/rancher/k3s/registries.yaml
# 同时可以设置多个mirrors地址
# 可以对mirrors设置权限和证书
mirrors:
  "172.31.6.200:5000":
    endpoint:
      - "http://172.31.6.200:5000"
      - "http://x.x.x.x:5000"
      - "http://y.y.y.y:5000"
  "rancher.ksd.top:5000":
    endpoint:
      - "http://172.31.6.200:5000"
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"

configs:
  "172.31.6.200:5000":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.escapelife.site.cert
      key_file: /home/ubuntu/harbor2.escapelife.site.key
      ca_file: /home/ubuntu/ca.crt
# 镜像都是从同一个仓库获取到的
$ sudo systemctl restart k3s.service
$ sudo crictl pull 172.31.6.200:5000/library/alpine
$ sudo crictl pull rancher.ksd.top:5000/library/alpine
```



这里我们介绍下，如何使用 `TLS` 配置。

::: details 展开

```bash

# 证书颁发机构颁发的证书
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor.escapelife.site":
    endpoint:
      - "https://harbor.escapelife.site"
configs:
  "harbor.escapelife.site":
    auth:
      username: admin
      password: Harbor@12345
EOF

sudo systemctl restart k3s

# 自签名证书
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor2.escapelife.site":
    endpoint:
      - "https://harbor2.escapelife.site"
configs:
  "harbor2.escapelife.site":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.escapelife.site.cert
      key_file:  /home/ubuntu/harbor2.escapelife.site.key
      ca_file:   /home/ubuntu/ca.crt
EOF

sudo systemctl restart k3s

# 不使用TLS证书
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"
EOF

sudo systemctl restart k3s
```

:::



`K3s` 将会在 `/var/lib/rancher/k3s/agent/etc/containerd/config.toml` 中为 `containerd` 生成 `config.toml`。如果要对这个文件进行高级设置，你可以在同一目录中创建另一个名为  `config.toml.tmpl` 的文件，此文件将会代替默认设置。

```bash

# 完整示例
cat >> /etc/rancher/k3s/registries.yaml
mirrors:
  "harbor.escapelife.site":
     endpoint:
     - "https://harbor.escapelife.site"
  "harbor2.escapelife.site":
     endpoint:
     - "https://harbor2.escapelife.site"
  "172.31.19.227:5000":
     endpoint:
     - "http://172.31.19.227:5000"
  "docker.io":
     endpoint:
     - "https://fogjl973.mirror.aliyuncs.com"
     - "https://registry-1.docker.io"

configs:
  "harbor.escapelife.site":
     auth:
       username: admin
       password: Harbor@12345

  "harbor2.escapelife.site":
     auth:
       username: admin
       password: Harbor@12345
     tls:
       cert_file: /home/ubuntu/harbor2.escapelife.site.cert
       key_file:  /home/ubuntu/harbor2.escapelife.site.key
       ca_file:   /home/ubuntu/ca.crt
```



## 安装事项 - 注意事项

**Helm**

+ 如果需要使用 `helm` 操作 `K3s` 集群，需要创建 `~/.kube/conf` 目录
+ 需要执行 `cp /etc/rancher/k3s/k3s.yaml ~/.kube/config` 命令



**自动部署的清单**

+ 将由 `rancher/helm-controller` 在运行时安装
+ 目录路径：`/var/lib/rancher/k3s/server/manifests`
+ 目录下面的每个 `yaml` 就代表这个一个需要启动的服务

对于我们希望使用的组件，可以在启动的时候禁用默认组件，在手动部署你需要的一些组件(通常是放到一个指定目录下面，随着服务启动自动拉起)，从而达到灵活使用的目的。

```bash
# 查看所有Pod服务
# 比如helm/coredns也不是自带的就是通过这个方式创建的
sudo kubectl get pods -A
```



**注册 Agent 节点**

1. 工作节点密码存储：`/etc/rancher/node/password`
2. 主节点的密码存储：`/var/lib/rancher/k3s/server/cred/node-passwd`

在 `agent` 节点运行注册命令，会和 `server` 节点发起 `websocket` 连接，然后会在工作节点上面创建一个随机的密码。然后会拿着这个密码和工作节点的主机名，发送给主节点。然后主节点会将这个信息在保存(`k8s secrets`)起来，随后的任何尝试都必须使用相同的密码。

```bash
# 工作节点的密码信息(password+hostname)
sudo cat /etc/rancher/node/password

# 查看主节点的密码信息
# https://docs.rancher.cn/docs/k3s/architecture/_index#注册-agent-节点
sudo kubectl get secret k3s2.node-password.k3s -o yaml -n kube-system

# 可以查看日志信息验证这个信息的存在
sudo tail -200f /var/log/syslog | grep k3s

# 发现节点信息提示NotReady状态
# 可以尝试删除节点的密码存储信息，之后会自动获取新的
sudo kubectl delete secret k3s2.node-password.k3s -n kube-system
```



**自定义存储类型**

集群启动之后，默认会启动一个 `local-path` 的组件，用于提供服务挂载存储使用，其默认以 `PVC` 的形式。之后，将其存储在 `/var/lib/rancher/k3s/server/storageclass` 目录下面。

```bash
# 查看组件
sudo kubectl get pods -A

# 查看对应存储
sudo kubectl get storageclass

# 可以使用参数修改默认存储地址
# --default-local-storage-path&nbsp;value
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--etcd-snapshot-schedule-cron *&nbsp;*/5&nbsp;*&nbsp;*&nbsp;*' \
    sh -
```



## K3s 集群升级

> **手动升级 + 自动升级**

当升级 `K3s` 时，`K3s` 服务会重启或停止，但 `K3s` 容器会继续运行。要停止所有的 `K3s` 容器并重置容器的状态，可以使用  `k3s-killall.sh` 脚本。 `killall` 脚本清理容器、`K3s` 目录和网络组件，同时也删除了 `iptables` 链和所有相关规则。集群数据不会被删除。



**手动升级 - 使用安装脚本升级 K3s**

你可以通过使用安装脚本升级 `K3s`，或者手动安装所需版本的二进制文件。

```bash
# 升级到最新stable版本
curl -sfL https://get.k3s.io | sh -

# 升级到latest版本
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -

# 升级到v1.20的最新版本
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL="v1.20" sh -

# 升级到指定版本
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -
```

> 注意和docker的区别，关于版本是 `+` 还是 `-`



**手动升级 - 使用二进制文件手动升级 K3s**

你可以通过使用安装脚本升级 `K3s`，或者手动安装所需版本的二进制文件。

```bash
# 从发布下载所需版本的K3s二进制文件
https://github.com/rancher/k3s/releases

# 将下载的二进制文件复制到/usr/local/bin/k3s
$ mv ./k3s /usr/local/bin/k3s

# 停止旧的K3s二进制文件
$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL="v1.20" sh -

# 启动新的K3s二进制文件
$ curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -
```

你可以使用 `Rancher` 的 `system-upgrad-controller` 来管理 `K3s` 集群升级。这是一种 `Kubernetes` 原生的集群升级方法。它利用自定义资源定义(`CRD`)、计划和控制器，根据配置的计划安排升级。

控制器通过监控计划和选择要在其上运行升级 `job` 的节点来调度升级，计划通过标签选择器定义哪些节点应该升级。当一个 `job` 成功运行完成后，控制器会给它运行的节点打上相应的标签。



**自动升级 - 使用二进制文件手动升级 K3s**

+ k3s-upgrade：https://github.com/k3s-io/k3s-upgrade
+ system-upgrade-controller：https://github.com/rancher/system-upgrade-controller

```bash
# 将system-upgrade-controller安装到您的集群中
kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml
```

::: details 配置计划

```bash
# 配置计划
# 建议您最少创建两个计划
# 升级server节点的计划和升级agent节点的计划

# Server plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: server-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master # 选择主节点
      operator: In
      values:
      - "true"
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1

# Agent plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: agent-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master # 选择工作节点
      operator: DoesNotExist
  prepare:
    args:
    - prepare
    - server-plan
    image: rancher/k3s-upgrade
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1
# 自动升级到最新版本(不指定版本)
apiVersion: upgrade.cattle.io/v1
kind: Plan
...
spec:
  ...
  upgrade:
    image: rancher/k3s-upgrade
  channel: https://update.k3s.io/v1-release/channels/stable
```

:::

![image-20221126000754469](http://sm.nsddd.top/smimage-20221126000754469.png)





## K3s 备份恢复

> **SQLite + etcd + 外部数据存储**

**使用嵌入式 SQLite 数据存储进行备份和恢复**

```bash
# 方式1：备份/恢复数据目录

# 备份
cp -rf /var/lib/rancher/k3s/server/db /opt/db

# 恢复
systemctl stop k3s
rm -rf /var/lib/rancher/k3s/server/db
cp -rf /opt/db /var/lib/rancher/k3s/server/db
systemctl start k3s


# 方式2：通过 SQLite cli

# 备份
sqlite3 /var/lib/rancher/k3s/server/db/state.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter ".help" for usage hints.
sqlite> .backup "/opt/kine.db"
sqlite> .exit

# 恢复
sudo systemctl stop k3s

sqlite3 /var/lib/rancher/k3s/server/db/state.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter ".help" for usage hints.
sqlite> .restore '/opt/kine.db'
sqlite> .exit

sudo systemctl start k3s
```

当使用外部数据存储时，备份和恢复操作是在 `K3s` 之外处理的。数据库管理员需要对外部数据库进行备份，或者从快照或转储中进行恢复。我们建议将数据库配置为执行定期快照。



**使用外部数据存储进行备份和恢复**

```bash
# 备份
mysqldump -uroot -p --all-databases --master-data > k3s-dbdump.db

# 恢复
systemctl stop k3s
mysql -uroot -p  < k3s-dbdump.db
systemctl start k3s
```



**使用嵌入式 etcd 数据存储进行备份和恢复**

```bash
# 创建快照(K3s默认启用快照)
# 快照目录默认: /var/lib/rancher/k3s/server/db/snapshots

# 要配置快照间隔或保留的快照数量
--etcd-disable-snapshots       禁用自动etcd快照
--etcd-snapshot-schedule-cron  定时快照的时间点；认值为每12小时触发一次
--etcd-snapshot-retention      保留的快照数量；默认值为5
--etcd-snapshot-dir            保存数据库快照的目录路径
--cluster-reset                忘记所有的对等体；成为新集群的唯一成员
--cluster-reset-restore-path   要恢复的快照文件的路径
```



当 `K3s` 从备份中恢复时，旧的数据目录将被移动到`/var/lib/rancher/k3s/server/db/etcd-old/`。然后 `K3s` 会尝试通过创建一个新的数据目录来恢复快照，然后从一个带有一个 `etcd` 成员的新 `K3s` 集群启动 `etcd`。

```bash
# 从快照恢复集群
# 使用--cluster-reset选项运行K3s
# 同时给出--cluster-reset-restore-path
./k3s server \
    --cluster-reset \
    --cluster-reset-restore-path=<PATH-TO-SNAPSHOT>
```



## K3s 卷和存储

> **介绍了如何通过 local storage provider 或 Longhorn 来设置持久存储。**

当部署一个需要保留数据的应用程序时，你需要创建持久存储。持久存储允许您从运行应用程序的 `pod` 外部存储应用程序数据。即使应用程序的 `pod` 发生故障，这种存储方式也可以使您维护应用程序数据。



**设置 Local Storage Provider 支持**

`K3s` 自带 `Rancher` 的 `Local Path Provisioner`(`LPP`)，这使得能够使用各自节点上的本地存储来开箱即用地创建 `pvc`。根据用户配置，`LPP` 将自动在节点上创建基于 `hostPath` 的持久卷。它利用了 `K8s` 的 `Local Persistent Volume` 特性引入的特性，但它比 `K8s` 中内置的  `local pv` 特性更简单的解决方案。

::: details pvc.yaml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-path-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 2Gi

# pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: local-path-pvc
```

:::

```bash
# 应用yaml服务
kubectl create -f pvc.yaml pod.yaml

# 确认PV和PVC已创建
kubectl get pv
kubectl get pvc
```



**设置 Longhorn 支持**

`K3s` 支持 `Longhorn`(是 `K8s` 的一个开源分布式块存储系统)。

::: details   pvc.yaml

```yaml

# 安装Longhorn
# 将被安装在命名空间longhorn-system中
$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml

# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi

# pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
```

:::

```bash
# 应用yaml服务
kubectl create -f pvc.yaml pod.yaml

# 确认PV和PVC已创建
kubectl get pv
kubectl get pvc
```





## K3s 网络相关

**CoreDNS**

`CoreDNS` 是在 `agent` 节点启动时部署的。要禁用，请在每台服务器上运行 `--disable coredns` 选项。如果你不安装 `CoreDNS`，你将需要自己安装一个集群 `DNS` 提供商。

```bash
# 如何修改coredns参数
# /var/lib/rancher/k3s/server/manifests/coredns.yaml
# 该文件重启K3s服务的话会导致coredns配置重新初始化
1.将coredns.yaml保存到其他目录
2.通过 --disable coredns 禁用coredns
3.复制coredns.yaml到/var/lib/rancher/k3s/server/manifests/目录并修改参数
```



**Traefik Ingress Controller**

启动 `server` 时，默认情况下会部署 `Traefik`，对该文件的任何修改都会以类似 `kubectl apply` 的方式自动部署到 `Kubernetes` 中，将使用主机上的 `80` 和 `443` 端口。

```yaml
# 操作和上面基本是一致的
# 请使用 --disable traefik 选项启动每个server
# /var/lib/rancher/k3s/server/manifests/traefik.yaml
# 如何启用 treafik2 dashboard
# http://traefik.example.com/dashboard

# Note: in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first.
# To create an encoded user:password pair, the following command can be used:
# htpasswd -nb admin admin | openssl base64

apiVersion: v1
kind: Secret
metadata:
  name: authsecret
  namespace: default
data:
  users: |2
    YWRtaW46JGFwcjEkLkUweHd1Z0EkUjBmLi85WndJNXZWRFMyR2F2LmtELwoK

---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: traefik-dashboard
spec:
  routes:
    - match: Host(`traefik.example.com`) && (PathPrefix(`/api`) || PathPrefix(`/dashboard`))
      kind: Rule
      services:
        - name: api@internal
          kind: TraefikService
      middlewares:
        - name: auth

---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: auth
spec:
  basicAuth:
    secret: authsecret # Kubernetes secret named "secretName"
```





**Service Load Balancer**

`K3s` 提供了一个名为 `Klipper Load Balancer` 的负载均衡器，它可以使用可用的主机端口。允许创建 `LoadBalancer` 类型的 `Service`，但不包括 `LB` 的实现。某些 `LB` 服务需要云提供商，例如 `Amazon EC2`。相比之下，`K3s service LB` 使得可以在没有云提供商的情况下使用 `LB` 服务。





## helm(k3s)

**K3s 与 Helm**

> **`Helm`** **是** **`Kubernetes`** **的包管理工具！**

`Helm` 是 `Kubernetes` 的包管理工具。`Helm Chart` 为 `Kubernetes YAML` 清单文件提供了模板化语法，可以通过 `Helm` 安装对应的 `chart`。`K3s` 不需要任何特殊的配置就可以使用 `Helm` 命令行工具。



**自动部署 Helm charts**

在 `/var/lib/rancher/k3s/server/manifests` 中找到的任何 `Kubernetes` 清单将以类似 `kubectl apply` 的方式自动部署到 `K3s`。以这种方式部署的 `manifests` 是作为 `AddOn` 自定义资源来管理的。你会发现打包组件的 `AddOns`，如 `CoreDNS`、`Local-Storage` 等。`AddOns` 是由部署控制器自动创建的，并根据它们在 `manifests` 目录下的文件名命名。

```bash
# 查看运行AddOn资源
$ kubectl get addon -A

# 也可以将Helm-Chart作为AddOns部署
https://github.com/rancher/helm-controller/
```

![image-20221126114506014](http://sm.nsddd.top/smimage-20221126114506014.png)



**使用 Helm CRD**

`HelmChart CRD` 捕获了大多数你通常会传递给 `helm` 命令行工具的选项。下面是一个例子，说明如何从默认的 `Chart` 资源库中部署 `Grafana`，覆盖一些默认的 `Chart` 值。请注意，`HelmChart` 资源本身在 `kube-system` 命名空间，但 `Chart` 资源将被部署到 `monitoring` 命名空间。

```yaml

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: grafana
  namespace: kube-system
spec:
  chart: stable/grafana
  targetNamespace: monitoring
  set:
    adminPassword: "NotVerySafePassword"
  valuesContent: |-
    image:
      tag: master
    env:
      GF_EXPLORE_ENABLED: true
    adminUser: admin
    sidecar:
      datasources:
        enabled: true
```



## K3s 高级选项

**证书轮换**

默认情况下，`K3s` 的证书在 `12` 个月内过期。如果证书已经过期或剩余的时间不足 `90` 天，则在 `K3s` 重启时轮换证书。

**查询 k3s 证书过期时间：**

```bash
root@cubmaster01:/var/lib/rancher/k3s/server/manifests#  for i in `ls /var/lib/rancher/k3s/server/tls/*.crt`; \
>   do \
>     echo $i;\
>     openssl x509 -enddate -noout -in $i; \
>   done
/var/lib/rancher/k3s/server/tls/client-admin.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/client-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-k3s-cloud-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-k3s-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-kube-proxy.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-scheduler.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/request-header-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/server-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt
notAfter=Nov 25 14:31:20 2023 GMT
```

**修改和重启：**

```bash
# 修改系统时间为证书过期前90天或证书过期后
timedatectl set-ntp no
date -s 20220807

# 重启K3s服务
service k3s restart
```



**Red Hat 和 CentOS 的额外准备**

建议运行以下命令，关闭 `firewalld` 防火墙。

```bash
sudo systemctl disable firewalld --now
```



## 所遇到的问题

+ 关于 k3s issue
+ 关于 讨论



::: tip 
更多的节点和安装选择问题看下半部分（下一节）

> 1.master节点的防火墙全部关掉，要不然worker可能连不上master

```text
# systemctl stop firewalld && systemctl disable firewalld && iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

> 2.有些.worker可能连不上master，可能有些worker的iptables表混乱了

```text
# systemctl stop k3s-agent
# systemctl stop docker
# iptables -F && iptables -t nat -F  && iptables -t mangle -F
# systemctl start docker
# systemctl stop k3s-agent
```

> 3.有些主机无法根据上面的命令下载docker，提示证书过期

```text
adding repo from: https://mydream.ink/utils/container/docker-ce.repo
grabbing file https://mydream.ink/utils/container/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
Could not fetch/save url https://mydream.ink/utils/container/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] curl#60 - "Peer's Certificate has expired."

因为这些主机的时间不正确，用ntp同步时间即可
# 安装ntp服务器
# yum install -y ntp
# 与一个已知的时间服务器同步
# ntpdate time.nist.gov
# 删除本地时间并设置时区为上海
# rm -rf /etc/localtime
# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
```

> 4.master 配置 --write-kubeconfig

```text
--write-kubeconfig ~/.kube/config效果为将配置文件写到k8s默认会用的位置，而不是k3s默认的位置/etc/rancher/k3s/k3s.yaml。后者会导致istio、helm需要额外设置或无法运行。
```

> 5.命令补全

```text
# yum install -y bash-completion
# source /usr/share/bash-completion/bash_completion
# source <(kubectl completion bash)
# echo "source <(kubectl completion bash)" >> ~/.bashrc
```

> 6.提示需要安装相关软件，根据提示进行安装

```text
# yum install -y k3s-selinux-0.1.1-rc1.el7.noarch.rpm
# yum install -y container-selinux selinux-policy-base
```

:::





## END 链接

::: tip about k3s

+ K3s 中文文档 - 国外：https://mirror.rancher.cn/

+ K3s 中文文档 - 国内：https://docs.rancher.cn/k3s/
+ K3s 国内镜像站 - 加速：https://mirror.rancher.cn/
+ K3s 系列教程 - 官方制作：https://github.com/kingsd041/k3s-tutorial



**代码地址：**

+ K3s 仓库地址 - Github：https://github.com/k3s-io



**扩展(k3d)：**

+ [vscode-k3d](https://github.com/inercia/vscode-k3d/)：VSCode 扩展，用于处理 VSCode 中的 k3d 集群
+ [k3x](https://github.com/inercia/k3x)：k3d 的图形接口（用于 Linux）。
+ [AbsaOSS/k3d-action](https://github.com/AbsaOSS/k3d-action)：完全可定制的 GitHub Action 来运行轻量级 Kubernetes 集群。
+ [AutoK3s](https://github.com/cnrancher/autok3s)：一个轻量级工具，可以帮助在任何地方运行 K3s，包括 k3d provider。
+ [nolar/setup-k3d-k3s](https://github.com/nolar/setup-k3d-k3s)：为 GitHub Actions 设置 K3d/K3s。



**周边项目：**

+ **K3s 周边项目 - k3os**：https://github.com/rancher/k3os

  完全基于 `K8S` 管理的轻量级操作系统

+ **K3s 周边项目 - autok3s**：https://github.com/cnrancher/autok3s

  用于简化 `K3s` 集群部署和管理的轻量级工具

  即在阿里云和 `aws` 等云服务器上面部署 `k3s`

+ **K3s 周边项目 - k3d**：https://github.com/cnrancher/autok3s

  可以在 `k3d` 创建容器化的 `k3s` 集群  k3d 是一个轻量级包装器，用于在 docker 中运行[k3s](https://github.com/rancher/k3s)（Rancher Lab 的最小 Kubernetes 发行版）

  可以使用容器在单台计算机上启动多节点 `k3s` 集群

+ **K3s 周边项目 - harvester**：https://github.com/harvester/harvester

  基于 `K8S` 构建的开源超融合基础架构(`HCI`)软件

  旨在替换 `vSphere` 和 `Nutanix` 的开源替代方案

+ **K3s 周边项目 - octopus**：https://github.com/cnrancher/octopus

  主要用于边缘计算相关

  用于 `K8S` 和 `k3s` 的轻量级云原生设备管理系统

  集群可以将边缘设备作为自定义 `k8s` 资源进行管理

:::



**about：**

<ul><li><div><a href = '13.md' style='float:left'>⬆️上一节🔗  </a><a href = '15.md' style='float: right'>  ️下一节🔗</a></div></li></ul>

+ [Ⓜ️回到目录🏠](../README.md)

+ [**🫵参与贡献💞❤️‍🔥💖**](https://nsddd.top/archives/contributors))

+ ✴️版权声明 &copy; ：本书所有内容遵循[CC-BY-SA 3.0协议（署名-相同方式共享）&copy;](http://zh.wikipedia.org/wiki/Wikipedia:CC-by-sa-3.0协议文本) 

