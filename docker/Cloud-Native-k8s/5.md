+ [author](http://nsddd.top)

# 第5节 集群搭建

<div><a href = '4.md' style='float:left'>⬆️上一节🔗  </a><a href = '6.md' style='float: right'>  ⬇️下一节🔗</a></div>
<br>

> ❤️💕💕新时代拥抱云原生，云原生具有环境统一、按需付费、即开即用、稳定性强特点。Myblog:[http://nsddd.top](http://nsddd.top/)

---
[[toc]]

[TOC]

## 购买三台机器

> 在青鸟云上创建账户，并且联系客户申请IP扩容为三个
>
> ![image-20221018114541669](http://sm.nsddd.top/smimage-20221018114541669.png)



### 推荐使用一个远程ssh工具

> 支持批量的输入，很适合集群的搭建~

+ [x] [electerm开源地址](https://electerm.github.io/electerm/)

![image-20221018122201424](http://sm.nsddd.top/smimage-20221018122201424.png)



### 创建私有网络

![image-20221018142107989](http://sm.nsddd.top/smimage-20221018142107989.png)

![image-20221018142057391](http://sm.nsddd.top/smimage-20221018142057391.png)



### 使用交换机划分为隔离网络的小区域

![image-20221018142311011](http://sm.nsddd.top/smimage-20221018142311011.png)



### 过程截图

![image-20221018143420128](http://sm.nsddd.top/smimage-20221018143420128.png)



> ⚠️ 上面一定要注意：内网的范围一定要选择`172.31.0.24/24`
>
> 因为后面安装docker的时候，docker占了`172.17`占了



### 搭建成功

![image-20221018143510468](http://sm.nsddd.top/smimage-20221018143510468.png)



### 显示图形化界面

> 可以看到`vpc`和防火墙以及三台主机之间的关联。

![image-20221018143709858](http://sm.nsddd.top/smimage-20221018143709858.png)



### 必须要打开安全组的组内通信

> **只有打开组内通信**，防火墙即使不开端口也是可以互相访问的~

![image-20221018143918187](http://sm.nsddd.top/smimage-20221018143918187.png)



### vpc设置端口转化

> vpc默认免费的是不会提供公网IP的，因为现在公网的IP紧缺，所以vpc在一定程度上需要满足这个条件，使用vpc的端口转化，可以帮助我们解决问题。这是因为vpc外网端口可以自定义。
>
> ⚠️ 注意要开放对应的安全组

| 类型     | ID                                                           | 名称 | 角色 | 状态   | IP 地址     | IPv6 地址 | DHCP 选项 | 操作     |
| :------- | :----------------------------------------------------------- | :--- | :--- | :----- | :---------- | :-------- | :-------- | :------- |
| 云服务器 | [i-jj9igf99](https://console.qingcloud.com/sh1/instances/instances/i-jj9igf99/) |      |      | 运行中 | 192.168.0.2 |           |           | [ 修改 ] |
| 云服务器 | [i-hdazrzq7](https://console.qingcloud.com/sh1/instances/instances/i-hdazrzq7/) |      |      | 运行中 | 192.168.0.3 |           |           | [ 修改 ] |
| 云服务器 | [i-bujgg88x](https://console.qingcloud.com/sh1/instances/instances/i-bujgg88x/) |      |      | 运行中 | 192.168.0.4 |           |           | [ 修改 ] |

![image-20221018151846925](http://sm.nsddd.top/smimage-20221018151846925.png)



### 准备链接

> 我开始也设置了一些基础的脚本，方便ce'shi

![image-20221018152452648](http://sm.nsddd.top/smimage-20221018152452648.png)



## docker安装

### 1、移除以前docker相关包

```bash
sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
```



### 2、配置yum源

```bash
sudo yum install -y yum-utils
sudo yum-config-manager \
	--add-repo \
	http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```



### 3、安装docker

```bash
sudo yum install -y docker-ce docker-ce-cli containerd.io


#以下是在安装k8s的时候使用
yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7  containerd.io-1.4.6
```



### 4、启动

```bash
systemctl enable docker --now
```



### 5、配置加速

这里额外添加了docker的生产环境核心配置cgroup

```bash
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://82m9ar63.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```



## kubeadm创建集群预备环境

## 安装kubeadm

+ 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令


+ 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)


+ 2 CPU 核或更多（我测试的 2核2G 的很卡）


+ 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)

+ **设置防火墙放行规则**

+ 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见[这里](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address)了解更多详细信息。


+ 主机名：`[root@i-0xkmp3n1 /]# ` 一般不会重复     **设置不同hostname**

+ 开启机器上的某些端口。请参见[这里](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports) 了解更多详细信息。--  **内网互信**

+ 禁用交换分区。为了保证 kubelet 正常工作，你 **必须** 禁用交换分区。 **永久关闭**



### 设置所有机器的主机名称

```bash
hostnamectl set-hostname k8s-master # 第一台机器
hostnamectl set-hostname k8s-node2 # 第二台机器
hostnamectl set-hostname k8s-node3 # 第三台机器
```



### 将 SELinux 设置为 permissive 模式

> 将 SELinux 设置为 permissive 模式（相当于将其禁用）

```bash
sudo setenforce 0  #临时
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```



### 关闭所有机器的交换分区

> 查看分区：`free -m`

```bash
#关闭swap
swapoff -a  #临时
sed -ri 's/.*swap.*/#&/' /etc/fstab #永久
```



### 允许 iptables 检查桥接流量
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system  # 让配置生效
```



## 安装集群的三大件

我们上一节看到了马车的三大件，我们集群也是有三大件的😂😂 

集群的搭建也是很有意思的，和redis的三主三从，Hadoop的搭建类似

>  安装kubelet、kubeadm、kubectl

```bash
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF


sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes   # 安装三大件

sudo systemctl enable --now kubelet   # 所有机器启动kublet
```

> 需要注意的是：kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环



## 使用kubeadm引导集群

1. 下载各个机器需要的镜像
2. 初始化主结点
3. 根据提示继续
4. 加入node结点

> 其实在`k8s`中，除了`kubelet`，剩下的镜像都是以容器的方式运行。`kubelet`类似于厂长，我们为了避免下载中断，所以准备的是脚本。

### 下载各个机器需要的镜像

```bash
sudo tee ./images.sh <<-'EOF'
#!/bin/bash
images=(
kube-apiserver:v1.20.9
kube-proxy:v1.20.9
kube-controller-manager:v1.20.9
kube-scheduler:v1.20.9
coredns:1.7.0
etcd:3.4.13-0
pause:3.2
)
for imageName in ${images[@]} ; do
docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
done
EOF
   
chmod +x ./images.sh && ./images.sh  #授予权限并且执行脚本
```

> 大概等了一会会就可以检索了
>
> ```
> docker images
> ```
>
> ![image-20221018173701125](http://sm.nsddd.top/smimage-20221018173701125.png)



### 初始化主结点

> ⚠️ 下面一定要按照自己的情况修改
>
> 使用`ip addr(ip a)`查看自己的内网地址。
>
> ```
> 192.168.0.2
> ```
>
> ![image-20221018174044509](http://sm.nsddd.top/smimage-20221018174044509.png)

```bash
#所有机器添加master域名映射，以下需要修改为自己的
echo "192.168.0.2  cluster-endpoint" >> /etc/hosts  # 每一个结点都需要输入
# 需要每一个结点能ping通

#主节点初始化 --apiserver-advertise-address改为master结点ip
kubeadm init \
--apiserver-advertise-address=192.168.0.2 \  
--control-plane-endpoint=cluster-endpoint \
--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
--kubernetes-version v1.20.9 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=192.168.0.0/16

#所有网络范围不重叠
```

📜 对上面的解释：

> 一定要注意每一个结点都需要能 `ping`通`cluster-endpoint`
>
> 主节点初始化：注意版本和网络范围的配置，这个挺麻烦的后面除非特别深入这一块，否则不要改。
>
> `--apiserver-advertise-address`一定要改成自己的`master`结点的`ip`
>
> **如果要改：**
>
> ```
> --service-cidr=10.96.0.0/16 \
> --pod-network-cidr=192.168.0.0/16
> ```
>
> 所有网络范围一定不能重叠！！！
>
> `--apiserver-advertise-address` 可以设置 control-plane 节点 API server 的 通告地址。
> `--control-plane-endpoint` 可以设置所有control-plane 节点的 shared endpoint。
> `--control-plane-endpoint` 允许IP地址，也允许DNS names。请联系管理员设置DNS和IP的映射。

```

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \
    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \
    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3
```

**继续：**

```bash
#查看集群所有节点
kubectl get nodes

#根据配置文件，给集群创建资源
kubectl apply -f xxxx.yaml

#查看集群部署了哪些应用？
docker ps   ===   kubectl get pods -A

# 运行中的应用在docker里面叫容器，在k8s里面叫Pod
kubectl get pods -A
```



### 根据提示继续

master成功后提示如下：

![image-20221018185049721](http://sm.nsddd.top/smimage-20221018185049721.png)



1、设置`.kube/config`



2、安装网络组件

+ [x] [calico官网](https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-more-than-50-nodes)

```bash
curl https://docs.projectcalico.org/manifests/calico.yaml -O
kubectl apply -f calico.yaml
```



### 加入node节点

```bash
kubeadm join cluster-endpoint:6443 --token x5g4uy.wpjjdbgra92s25pp \
	--discovery-token-ca-cert-hash sha256:6255797916eaee52bf9dda9429db616fcd828436708345a308f4b917d3457a22
```

> 新令牌
>
> ```
> kubeadm token create --print-join-command
> ```
>
> ***高可用部署方式，也是在这一步的时候，使用添加主节点的命令即可***



## END 链接

<ul><li><div><a href = '4.md' style='float:left'>⬆️上一节🔗  </a><a href = '6.md' style='float: right'>  ️下一节🔗</a></div></li></ul>

+ [Ⓜ️回到目录🏠](../README.md)

+ [**🫵参与贡献💞❤️‍🔥💖**](https://nsddd.top/archives/contributors))

+ ✴️版权声明 &copy; ：本书所有内容遵循[CC-BY-SA 3.0协议（署名-相同方式共享）&copy;](http://zh.wikipedia.org/wiki/Wikipedia:CC-by-sa-3.0协议文本) 

